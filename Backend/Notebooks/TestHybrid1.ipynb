{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4c60b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ed8608a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ffb_1%_oer</th>\n",
       "      <th>import</th>\n",
       "      <th>export</th>\n",
       "      <th>production</th>\n",
       "      <th>end_stock</th>\n",
       "      <th>cpo_futures</th>\n",
       "      <th>usd_myr_rate</th>\n",
       "      <th>brent_oil_futures</th>\n",
       "      <th>soybean_futures</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_humidity</th>\n",
       "      <th>lag_1</th>\n",
       "      <th>lag_3</th>\n",
       "      <th>lag_7</th>\n",
       "      <th>rolling_mean_7</th>\n",
       "      <th>rolling_mean_30</th>\n",
       "      <th>rolling_std_7</th>\n",
       "      <th>rolling_std_30</th>\n",
       "      <th>pct_change_1</th>\n",
       "      <th>pct_change_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.2</td>\n",
       "      <td>81477</td>\n",
       "      <td>1680891</td>\n",
       "      <td>1737461</td>\n",
       "      <td>3002871</td>\n",
       "      <td>2204.0</td>\n",
       "      <td>4.1075</td>\n",
       "      <td>61.65</td>\n",
       "      <td>30.74</td>\n",
       "      <td>20.6</td>\n",
       "      <td>...</td>\n",
       "      <td>88.041667</td>\n",
       "      <td>21.25</td>\n",
       "      <td>20.85</td>\n",
       "      <td>20.60</td>\n",
       "      <td>20.992857</td>\n",
       "      <td>20.533333</td>\n",
       "      <td>0.212972</td>\n",
       "      <td>0.319032</td>\n",
       "      <td>-0.002353</td>\n",
       "      <td>0.029126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.3</td>\n",
       "      <td>81477</td>\n",
       "      <td>1680891</td>\n",
       "      <td>1737461</td>\n",
       "      <td>3002871</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>4.0960</td>\n",
       "      <td>61.89</td>\n",
       "      <td>30.48</td>\n",
       "      <td>47.5</td>\n",
       "      <td>...</td>\n",
       "      <td>90.083333</td>\n",
       "      <td>21.20</td>\n",
       "      <td>21.20</td>\n",
       "      <td>20.75</td>\n",
       "      <td>21.071429</td>\n",
       "      <td>20.576667</td>\n",
       "      <td>0.209875</td>\n",
       "      <td>0.332113</td>\n",
       "      <td>0.004717</td>\n",
       "      <td>0.026506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.3</td>\n",
       "      <td>94278</td>\n",
       "      <td>1324615</td>\n",
       "      <td>1544518</td>\n",
       "      <td>3056929</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>4.0960</td>\n",
       "      <td>62.75</td>\n",
       "      <td>30.21</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>89.958333</td>\n",
       "      <td>21.30</td>\n",
       "      <td>21.25</td>\n",
       "      <td>20.85</td>\n",
       "      <td>21.135714</td>\n",
       "      <td>20.620000</td>\n",
       "      <td>0.199404</td>\n",
       "      <td>0.339015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.3</td>\n",
       "      <td>94278</td>\n",
       "      <td>1324615</td>\n",
       "      <td>1544518</td>\n",
       "      <td>3056929</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>4.0960</td>\n",
       "      <td>62.75</td>\n",
       "      <td>30.21</td>\n",
       "      <td>4.7</td>\n",
       "      <td>...</td>\n",
       "      <td>90.083333</td>\n",
       "      <td>21.30</td>\n",
       "      <td>21.20</td>\n",
       "      <td>20.85</td>\n",
       "      <td>21.200000</td>\n",
       "      <td>20.660000</td>\n",
       "      <td>0.160728</td>\n",
       "      <td>0.346261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.3</td>\n",
       "      <td>94278</td>\n",
       "      <td>1324615</td>\n",
       "      <td>1544518</td>\n",
       "      <td>3056929</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>4.0960</td>\n",
       "      <td>62.75</td>\n",
       "      <td>30.21</td>\n",
       "      <td>13.2</td>\n",
       "      <td>...</td>\n",
       "      <td>89.125000</td>\n",
       "      <td>21.30</td>\n",
       "      <td>21.30</td>\n",
       "      <td>20.85</td>\n",
       "      <td>21.264286</td>\n",
       "      <td>20.690000</td>\n",
       "      <td>0.047559</td>\n",
       "      <td>0.361606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ffb_1%_oer  import   export  production  end_stock  cpo_futures  \\\n",
       "0        21.2   81477  1680891     1737461    3002871       2204.0   \n",
       "1        21.3   81477  1680891     1737461    3002871       2200.0   \n",
       "2        21.3   94278  1324615     1544518    3056929       2200.0   \n",
       "3        21.3   94278  1324615     1544518    3056929       2200.0   \n",
       "4        21.3   94278  1324615     1544518    3056929       2200.0   \n",
       "\n",
       "   usd_myr_rate  brent_oil_futures  soybean_futures  precipitation  ...  \\\n",
       "0        4.1075              61.65            30.74           20.6  ...   \n",
       "1        4.0960              61.89            30.48           47.5  ...   \n",
       "2        4.0960              62.75            30.21            7.0  ...   \n",
       "3        4.0960              62.75            30.21            4.7  ...   \n",
       "4        4.0960              62.75            30.21           13.2  ...   \n",
       "\n",
       "   avg_humidity  lag_1  lag_3  lag_7  rolling_mean_7  rolling_mean_30  \\\n",
       "0     88.041667  21.25  20.85  20.60       20.992857        20.533333   \n",
       "1     90.083333  21.20  21.20  20.75       21.071429        20.576667   \n",
       "2     89.958333  21.30  21.25  20.85       21.135714        20.620000   \n",
       "3     90.083333  21.30  21.20  20.85       21.200000        20.660000   \n",
       "4     89.125000  21.30  21.30  20.85       21.264286        20.690000   \n",
       "\n",
       "   rolling_std_7  rolling_std_30  pct_change_1  pct_change_7  \n",
       "0       0.212972        0.319032     -0.002353      0.029126  \n",
       "1       0.209875        0.332113      0.004717      0.026506  \n",
       "2       0.199404        0.339015      0.000000      0.021583  \n",
       "3       0.160728        0.346261      0.000000      0.021583  \n",
       "4       0.047559        0.361606      0.000000      0.021583  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load cleaned dataset\n",
    "df= pd.read_parquet(\"cleaned_data.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba8379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"ffb_1%_oer\"\n",
    "raw_features = [\"import\", \"export\", \"production\", \"end_stock\", \n",
    "                \"cpo_futures\", \"usd_myr_rate\", \"brent_oil_futures\", \n",
    "                \"soybean_futures\", \"precipitation\", \"avg_temperature\", \"avg_humidity\"]\n",
    "\n",
    "engineered_features = [\"lag_1\",\"lag_3\",\"lag_7\",\"rolling_mean_7\",\n",
    "                       \"rolling_mean_30\",\"rolling_std_7\",\"rolling_std_30\",\n",
    "                       \"pct_change_1\",\"pct_change_7\"]\n",
    "\n",
    "engineered_features_lstm = [\"lag_1\", \"rolling_mean_7\"]\n",
    "\n",
    "X = df[raw_features + engineered_features].values\n",
    "y = df[target_col].values.reshape(-1,1)\n",
    "\n",
    "\n",
    "#Splitting into train-validate-test dataa\n",
    "N = len(df)\n",
    "train_size = int(N * 0.7)   # 70% train\n",
    "val_size   = int(N * 0.2)  # 20% validation\n",
    "test_size  = N - train_size - val_size  # 10% test\n",
    "\n",
    "X_train_raw = X[:train_size]\n",
    "X_val_raw   = X[train_size:train_size+val_size]\n",
    "X_test_raw  = X[train_size+val_size:]\n",
    "\n",
    "y_train_raw = y[:train_size]\n",
    "y_val_raw   = y[train_size:train_size+val_size]\n",
    "y_test_raw  = y[train_size+val_size:]\n",
    "\n",
    "\n",
    "#scale data\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_train = scaler_x.fit_transform(X_train_raw)\n",
    "X_val   = scaler_x.transform(X_val_raw)\n",
    "X_test  = scaler_x.transform(X_test_raw)\n",
    "\n",
    "y_train = scaler_y.fit_transform(y_train_raw)\n",
    "y_val   = scaler_y.transform(y_val_raw)\n",
    "y_test  = scaler_y.transform(y_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27aa7085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_step_sequences(X, y, lookback, horizon):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(lookback, len(X) - horizon + 1):\n",
    "        Xs.append(X[i - lookback:i])\n",
    "        ys.append(y[i:i + horizon].ravel())  # collect next horizon steps\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "forecast_horizon = 7\n",
    "lookback = 90\n",
    "X_train_lstm, y_train_lstm = create_multi_step_sequences(X_train, y_train, lookback, forecast_horizon)\n",
    "X_val_lstm, y_val_lstm     = create_multi_step_sequences(X_val, y_val, lookback, forecast_horizon)\n",
    "X_test_lstm, y_test_lstm   = create_multi_step_sequences(X_test, y_test, lookback, forecast_horizon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd9d7b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User\\Desktop\\test2\\venv\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\test2\\venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - loss: 0.0347 - val_loss: 0.0039\n",
      "Epoch 2/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0103 - val_loss: 0.0024\n",
      "Epoch 3/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.0082 - val_loss: 0.0027\n",
      "Epoch 4/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.0070 - val_loss: 0.0019\n",
      "Epoch 5/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0056 - val_loss: 0.0035\n",
      "Epoch 6/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0051 - val_loss: 0.0023\n",
      "Epoch 7/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0047 - val_loss: 0.0025\n",
      "Epoch 8/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 0.0042 - val_loss: 0.0019\n",
      "Epoch 9/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.0042 - val_loss: 0.0013\n",
      "Epoch 10/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.0039 - val_loss: 0.0013\n",
      "Epoch 11/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.0037 - val_loss: 0.0013\n",
      "Epoch 12/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.0034 - val_loss: 0.0015\n",
      "Epoch 13/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0029 - val_loss: 6.8854e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.0029 - val_loss: 0.0020\n",
      "Epoch 15/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.0025 - val_loss: 0.0015\n",
      "Epoch 16/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.0027 - val_loss: 0.0014\n",
      "Epoch 17/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 18/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0023 - val_loss: 0.0013\n",
      "Epoch 19/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0026 - val_loss: 0.0014\n",
      "Epoch 20/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0022 - val_loss: 0.0011\n",
      "Epoch 21/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 0.0021 - val_loss: 8.4023e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 23/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 24/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - loss: 0.0020 - val_loss: 9.4685e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 0.0020 - val_loss: 8.6924e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 27/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 28/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0018 - val_loss: 0.0012\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "#Clear previous model\n",
    "K.clear_session()\n",
    "\n",
    "#fix random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(lookback, X_train.shape[1])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32),\n",
    "    Dropout(0.2),\n",
    "    Dense(forecast_horizon)  # output 7 values simultaneously\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "#EarlyStopping Callback\n",
    "early_stop = EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_lstm, y_train_lstm,\n",
    "    validation_data=(X_val_lstm, y_val_lstm),\n",
    "    epochs = 50,\n",
    "    batch_size = 32,\n",
    "    callbacks = [early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred_lstm = model.predict(X_test_lstm)\n",
    "\n",
    "# Shape is now (samples, horizon)\n",
    "y_pred_lstm_inv = scaler_y.inverse_transform(y_pred_lstm.reshape(-1,1)).reshape(y_pred_lstm.shape)\n",
    "y_test_lstm_inv = scaler_y.inverse_transform(y_test_lstm.reshape(-1,1)).reshape(y_test_lstm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93c2f5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_models = []\n",
    "y_preds_xgb = []\n",
    "xgb_features_idx = [(raw_features + engineered_features).index(f) for f in engineered_features ]\n",
    "\n",
    "X_train_xgb = X_train[:,xgb_features_idx]\n",
    "X_val_xgb = X_val[:,xgb_features_idx]\n",
    "X_test_xgb = X_test[:,xgb_features_idx]\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"eta\": 0.1,\n",
    "    \"max_depth\": 5\n",
    "}\n",
    "\n",
    "\n",
    "for step in range(forecast_horizon):\n",
    "    # target is shifted for each horizon step\n",
    "    y_train_step = y_train[lookback + step: len(y_train) - forecast_horizon + step + 1]\n",
    "    y_val_step   = y_val[lookback + step: len(y_val) - forecast_horizon + step + 1]\n",
    "    y_test_step  = y_test[lookback + step: len(y_test) - forecast_horizon + step + 1]\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train_xgb[lookback:len(y_train_step)+lookback], label=y_train_step)\n",
    "    dval   = xgb.DMatrix(X_val_xgb[lookback:len(y_val_step)+lookback], label=y_val_step)\n",
    "    dtest  = xgb.DMatrix(X_test_xgb[lookback:len(y_test_step)+lookback])\n",
    "\n",
    "    model_step = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=500,\n",
    "        evals=[(dtrain, \"train\"), (dval, \"val\")],\n",
    "        early_stopping_rounds=20,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    xgb_models.append(model_step)\n",
    "    y_preds_xgb.append(model_step.predict(dtest))\n",
    "\n",
    "y_pred_xgb = np.column_stack(y_preds_xgb)  # shape: (samples, horizon)\n",
    "y_pred_xgb_inv = scaler_y.inverse_transform(y_pred_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74151521",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_hybrid_inv = 0.5 * y_pred_lstm_inv + 0.5 * y_pred_xgb_inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "306034d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [365, 136]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 1. Compute validation RMSE per horizon\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m val_errors_lstm = [\u001b[43mroot_mean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_val_lstm\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mh\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_lstm\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mh\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m                    \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(forecast_horizon)]\n\u001b[32m      4\u001b[39m val_errors_xgb  = [root_mean_squared_error(y_val_lstm[:,h], y_pred_xgb[:,h])\n\u001b[32m      5\u001b[39m                    \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(forecast_horizon)]\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 2. Convert to normalized weights (lower error = higher weight)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\test2\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\test2\\venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:665\u001b[39m, in \u001b[36mroot_mean_squared_error\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Root mean squared error regression loss.\u001b[39;00m\n\u001b[32m    616\u001b[39m \n\u001b[32m    617\u001b[39m \u001b[33;03mRead more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    659\u001b[39m \u001b[33;03m0.822...\u001b[39;00m\n\u001b[32m    660\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    662\u001b[39m xp, _ = get_namespace(y_true, y_pred, sample_weight, multioutput)\n\u001b[32m    664\u001b[39m output_errors = xp.sqrt(\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m     \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mraw_values\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    667\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    668\u001b[39m )\n\u001b[32m    670\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(multioutput, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    671\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m multioutput == \u001b[33m\"\u001b[39m\u001b[33mraw_values\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\test2\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:191\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m global_skip_validation = get_config()[\u001b[33m\"\u001b[39m\u001b[33mskip_parameter_validation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m func_sig = signature(func)\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\test2\\venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:580\u001b[39m, in \u001b[36mmean_squared_error\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput)\u001b[39m\n\u001b[32m    530\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[32m    531\u001b[39m \n\u001b[32m    532\u001b[39m \u001b[33;03mRead more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    576\u001b[39m \u001b[33;03m0.825...\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    578\u001b[39m xp, _ = get_namespace(y_true, y_pred, sample_weight, multioutput)\n\u001b[32m    579\u001b[39m _, y_true, y_pred, sample_weight, multioutput = (\n\u001b[32m--> \u001b[39m\u001b[32m580\u001b[39m     \u001b[43m_check_reg_targets_with_floating_dtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m )\n\u001b[32m    584\u001b[39m output_errors = _average((y_true - y_pred) ** \u001b[32m2\u001b[39m, axis=\u001b[32m0\u001b[39m, weights=sample_weight)\n\u001b[32m    586\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(multioutput, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\test2\\venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:209\u001b[39m, in \u001b[36m_check_reg_targets_with_floating_dtype\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Ensures y_true, y_pred, and sample_weight correspond to same regression task.\u001b[39;00m\n\u001b[32m    161\u001b[39m \n\u001b[32m    162\u001b[39m \u001b[33;03mExtends `_check_reg_targets` by automatically selecting a suitable floating-point\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    205\u001b[39m \u001b[33;03m    correct keyword.\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    207\u001b[39m dtype_name = _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp=xp)\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m y_type, y_true, y_pred, sample_weight, multioutput = \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y_type, y_true, y_pred, sample_weight, multioutput\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\test2\\venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:114\u001b[39m, in \u001b[36m_check_reg_targets\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput, dtype, xp)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check that y_true, y_pred and sample_weight belong to the same regression task.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03mTo reduce redundancy when calling `_find_matching_floating_dtype`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    110\u001b[39m \u001b[33;03m    correct keyword.\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    112\u001b[39m xp, _ = get_namespace(y_true, y_pred, multioutput, xp=xp)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m y_true = check_array(y_true, ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=dtype)\n\u001b[32m    116\u001b[39m y_pred = check_array(y_pred, ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\test2\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:473\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    471\u001b[39m lengths = [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lengths)) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    474\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    475\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    476\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [365, 136]"
     ]
    }
   ],
   "source": [
    "# 1. Compute validation RMSE per horizon\n",
    "val_errors_lstm = [root_mean_squared_error(y_val_lstm[:,h], y_pred_lstm[:,h])\n",
    "                   for h in range(forecast_horizon)]\n",
    "val_errors_xgb  = [root_mean_squared_error(y_val_lstm[:,h], y_pred_xgb[:,h])\n",
    "                   for h in range(forecast_horizon)]\n",
    "\n",
    "# 2. Convert to normalized weights (lower error = higher weight)\n",
    "val_errors_lstm = np.array(val_errors_lstm)\n",
    "val_errors_xgb  = np.array(val_errors_xgb)\n",
    "weights_lstm = 1 / val_errors_lstm\n",
    "weights_xgb  = 1 / val_errors_xgb\n",
    "weights_sum = weights_lstm + weights_xgb\n",
    "weights_lstm /= weights_sum\n",
    "weights_xgb  /= weights_sum\n",
    "\n",
    "# 3. Combine predictions using weights\n",
    "y_pred_hybrid = weights_lstm * y_pred_lstm_inv + weights_xgb * y_pred_xgb_inv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69b21b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "Validation Weights (per horizon):\n",
      "H1: LSTM=0.51, XGB=0.49\n",
      "H2: LSTM=0.50, XGB=0.50\n",
      "H3: LSTM=0.50, XGB=0.50\n",
      "H4: LSTM=0.50, XGB=0.50\n",
      "H5: LSTM=0.51, XGB=0.49\n",
      "H6: LSTM=0.50, XGB=0.50\n",
      "H7: LSTM=0.51, XGB=0.49\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Get validation predictions ---\n",
    "# LSTM validation prediction\n",
    "y_pred_val_lstm = model.predict(X_val_lstm)\n",
    "y_pred_val_lstm_inv = scaler_y.inverse_transform(\n",
    "    y_pred_val_lstm.reshape(-1,1)\n",
    ").reshape(y_pred_val_lstm.shape)\n",
    "\n",
    "# XGBoost validation prediction\n",
    "y_preds_val_xgb = []\n",
    "for step, model_step in enumerate(xgb_models):\n",
    "    dval = xgb.DMatrix(X_val_xgb[lookback: len(y_val_lstm) + lookback])\n",
    "    y_preds_val_xgb.append(model_step.predict(dval))\n",
    "\n",
    "y_pred_val_xgb = np.column_stack(y_preds_val_xgb)\n",
    "y_pred_val_xgb_inv = scaler_y.inverse_transform(y_pred_val_xgb)\n",
    "\n",
    "# --- 2. Compute validation errors per horizon ---\n",
    "val_errors_lstm = [\n",
    "    root_mean_squared_error(y_val_lstm[:, h], y_pred_val_lstm_inv[:, h])\n",
    "    for h in range(forecast_horizon)\n",
    "]\n",
    "val_errors_xgb = [\n",
    "    root_mean_squared_error(y_val_lstm[:, h], y_pred_val_xgb_inv[:, h])\n",
    "    for h in range(forecast_horizon)\n",
    "]\n",
    "\n",
    "# --- 3. Convert errors into weights (lower error = higher weight) ---\n",
    "val_errors_lstm = np.array(val_errors_lstm)\n",
    "val_errors_xgb  = np.array(val_errors_xgb)\n",
    "\n",
    "weights_lstm = 1 / val_errors_lstm\n",
    "weights_xgb  = 1 / val_errors_xgb\n",
    "weights_sum  = weights_lstm + weights_xgb\n",
    "weights_lstm /= weights_sum\n",
    "weights_xgb  /= weights_sum\n",
    "\n",
    "print(\"Validation Weights (per horizon):\")\n",
    "for h in range(forecast_horizon):\n",
    "    print(f\"H{h+1}: LSTM={weights_lstm[h]:.2f}, XGB={weights_xgb[h]:.2f}\")\n",
    "\n",
    "# --- 4. Apply weights on test predictions ---\n",
    "y_pred_hybrid_inv = weights_lstm * y_pred_lstm_inv + weights_xgb * y_pred_xgb_inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c99c8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizon 1: RMSE=2.7453, MAE=2.2943, MAPE=4.91%, R^2=44.41%\n",
      "Horizon 2: RMSE=2.8986, MAE=2.3507, MAPE=5.00%, R^2=39.44%\n",
      "Horizon 3: RMSE=2.2018, MAE=1.7517, MAPE=3.76%, R^2=65.93%\n",
      "Horizon 4: RMSE=2.1854, MAE=1.7126, MAPE=3.68%, R^2=67.11%\n",
      "Horizon 5: RMSE=3.2030, MAE=2.6358, MAPE=5.63%, R^2=30.18%\n",
      "Horizon 6: RMSE=2.4596, MAE=1.9217, MAPE=4.14%, R^2=59.41%\n",
      "Horizon 7: RMSE=2.8016, MAE=2.1438, MAPE=4.59%, R^2=48.08%\n"
     ]
    }
   ],
   "source": [
    "for h in range(forecast_horizon):\n",
    "    rmse = root_mean_squared_error(y_test_lstm_inv[:, h], y_pred_lstm_inv[:, h])\n",
    "    mae = mean_absolute_error(y_test_lstm_inv[:, h], y_pred_lstm_inv[:, h])\n",
    "    mape = mean_absolute_percentage_error(y_test_lstm_inv[:, h], y_pred_lstm_inv[:, h])\n",
    "    r2 = r2_score(y_test_lstm_inv[:, h], y_pred_lstm_inv[:, h])\n",
    "    print(f\"Horizon {h+1}: RMSE={rmse:.4f}, MAE={mae:.4f}, MAPE={mape:.2%}, R^2={r2:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e9baa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizon 1: RMSE=0.4647, MAE=0.3564, MAPE=0.76%, R²=98.41%\n",
      "Horizon 2: RMSE=0.8813, MAE=0.7243, MAPE=1.59%, R²=94.40%\n",
      "Horizon 3: RMSE=1.2142, MAE=0.9919, MAPE=2.18%, R²=89.64%\n",
      "Horizon 4: RMSE=1.4142, MAE=1.1020, MAPE=2.46%, R²=86.23%\n",
      "Horizon 5: RMSE=1.6997, MAE=1.3338, MAPE=2.94%, R²=80.34%\n",
      "Horizon 6: RMSE=1.7817, MAE=1.3618, MAPE=3.02%, R²=78.70%\n",
      "Horizon 7: RMSE=1.9920, MAE=1.4474, MAPE=3.21%, R²=73.75%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "for h in range(forecast_horizon):\n",
    "    rmse = root_mean_squared_error(y_test_lstm_inv[:, h], y_pred_xgb_inv[:, h])\n",
    "    mae  = mean_absolute_error(y_test_lstm_inv[:, h], y_pred_xgb_inv[:, h])\n",
    "    mape = mean_absolute_percentage_error(y_test_lstm_inv[:, h], y_pred_xgb_inv[:, h])\n",
    "    r2   = r2_score(y_test_lstm_inv[:, h], y_pred_xgb_inv[:, h])\n",
    "    print(f\"Horizon {h+1}: RMSE={rmse:.4f}, MAE={mae:.4f}, MAPE={mape:.2%}, R²={r2:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab577543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizon 1: RMSE=1.2886, MAE=1.0560, MAPE=2.26%, R²=87.75%\n",
      "Horizon 2: RMSE=1.3437, MAE=1.0886, MAPE=2.33%, R²=86.99%\n",
      "Horizon 3: RMSE=1.0887, MAE=0.8830, MAPE=1.91%, R²=91.67%\n",
      "Horizon 4: RMSE=1.3269, MAE=1.0839, MAPE=2.35%, R²=87.88%\n",
      "Horizon 5: RMSE=1.6621, MAE=1.3169, MAPE=2.83%, R²=81.20%\n",
      "Horizon 6: RMSE=1.4714, MAE=1.1883, MAPE=2.58%, R²=85.48%\n",
      "Horizon 7: RMSE=1.7256, MAE=1.3515, MAPE=2.91%, R²=80.30%\n"
     ]
    }
   ],
   "source": [
    "for h in range(forecast_horizon):\n",
    "    rmse = root_mean_squared_error(y_test_lstm_inv[:, h], y_pred_hybrid_inv[:, h])\n",
    "    mae  = mean_absolute_error(y_test_lstm_inv[:, h], y_pred_hybrid_inv[:, h])\n",
    "    mape = mean_absolute_percentage_error(y_test_lstm_inv[:, h], y_pred_hybrid_inv[:, h])\n",
    "    r2   = r2_score(y_test_lstm_inv[:, h], y_pred_hybrid_inv[:, h])\n",
    "    \n",
    "    print(f\"Horizon {h+1}: RMSE={rmse:.4f}, MAE={mae:.4f}, MAPE={mape:.2%}, R²={r2:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1d6861e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_hybrid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m pred_lstm   = y_pred_lstm_inv[start_idx]\n\u001b[32m     12\u001b[39m pred_xgb    = y_pred_xgb_inv[start_idx]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m pred_hybrid = \u001b[43my_pred_hybrid\u001b[49m[start_idx]\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Build x-axis (Day 1 ... Day 7 relative to forecast start)\u001b[39;00m\n\u001b[32m     16\u001b[39m days = np.arange(\u001b[32m1\u001b[39m, forecast_horizon + \u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'y_pred_hybrid' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Choose a starting index in test set to plot (e.g. first forecast window)\n",
    "start_idx = 0\n",
    "\n",
    "# Actual data for the forecast horizon\n",
    "actual = y_test_lstm_inv[start_idx]\n",
    "\n",
    "# Predictions from each model\n",
    "pred_lstm   = y_pred_lstm_inv[start_idx]\n",
    "pred_xgb    = y_pred_xgb_inv[start_idx]\n",
    "pred_hybrid = y_pred_hybrid[start_idx]\n",
    "\n",
    "# Build x-axis (Day 1 ... Day 7 relative to forecast start)\n",
    "days = np.arange(1, forecast_horizon + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(days, actual, label=\"Actual\", marker=\"o\", linewidth=2, color=\"black\")\n",
    "plt.plot(days, pred_lstm, label=\"LSTM\", marker=\"o\", linestyle=\"--\")\n",
    "plt.plot(days, pred_xgb, label=\"XGBoost\", marker=\"o\", linestyle=\"--\")\n",
    "plt.plot(days, pred_hybrid, label=\"Hybrid\", marker=\"o\", linestyle=\"--\")\n",
    "\n",
    "plt.title(f\"Forecast vs Actual (Test Start Index: {start_idx})\")\n",
    "plt.xlabel(\"Forecast Horizon (Days Ahead)\")\n",
    "plt.ylabel(\"FFB OER (%)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b002ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Price: 38.58\n",
      "7-Day Forecast: [38.02784017 38.4066775  38.55591555 38.66990518 38.3741187  38.73831237\n",
      " 38.63097242]\n",
      "Average Forecast: 38.486248841849864\n",
      "Recommendation: NEUTRAL (Price change small: -0.24%)\n"
     ]
    }
   ],
   "source": [
    "# --- Recommendation System Layer ---\n",
    "\n",
    "# Use last actual price from test set as \"current\" reference\n",
    "current_price = y_test_lstm_inv[-1, 0]  \n",
    "\n",
    "# Hybrid 7-day forecast (latest window)\n",
    "latest_forecast = y_pred_hybrid_inv[-1]  # shape: (7,)\n",
    "\n",
    "# Compute average and % change\n",
    "avg_forecast = np.mean(latest_forecast)\n",
    "pct_change = ((avg_forecast - current_price) / current_price) * 100\n",
    "\n",
    "# Recommendation logic\n",
    "threshold = 2.0  # 2% threshold, tune this value\n",
    "if pct_change > threshold:\n",
    "    recommendation = f\"HOLD/WAIT (Price expected to rise by {pct_change:.2f}%)\"\n",
    "elif pct_change < -threshold:\n",
    "    recommendation = f\"SELL NOW (Price expected to drop by {pct_change:.2f}%)\"\n",
    "else:\n",
    "    recommendation = f\"NEUTRAL (Price change small: {pct_change:.2f}%)\"\n",
    "\n",
    "print(\"Current Price:\", current_price)\n",
    "print(\"7-Day Forecast:\", latest_forecast)\n",
    "print(\"Average Forecast:\", avg_forecast)\n",
    "print(\"Recommendation:\", recommendation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
