{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea1ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "946a8ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ffb_1%_oer</th>\n",
       "      <th>import</th>\n",
       "      <th>export</th>\n",
       "      <th>production</th>\n",
       "      <th>end_stock</th>\n",
       "      <th>cpo_futures</th>\n",
       "      <th>usd_myr_rate</th>\n",
       "      <th>brent_oil_futures</th>\n",
       "      <th>soybean_futures</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_humidity</th>\n",
       "      <th>lag_1</th>\n",
       "      <th>lag_3</th>\n",
       "      <th>lag_7</th>\n",
       "      <th>rolling_mean_7</th>\n",
       "      <th>rolling_mean_30</th>\n",
       "      <th>rolling_std_7</th>\n",
       "      <th>rolling_std_30</th>\n",
       "      <th>pct_change_1</th>\n",
       "      <th>pct_change_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.30</td>\n",
       "      <td>81477</td>\n",
       "      <td>1680891</td>\n",
       "      <td>1737461</td>\n",
       "      <td>3002871</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>4.0960</td>\n",
       "      <td>61.89</td>\n",
       "      <td>30.48</td>\n",
       "      <td>47.5</td>\n",
       "      <td>...</td>\n",
       "      <td>90.083333</td>\n",
       "      <td>21.2</td>\n",
       "      <td>21.20</td>\n",
       "      <td>20.75</td>\n",
       "      <td>21.071429</td>\n",
       "      <td>20.576667</td>\n",
       "      <td>0.209875</td>\n",
       "      <td>0.332113</td>\n",
       "      <td>0.004717</td>\n",
       "      <td>0.026506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.30</td>\n",
       "      <td>94278</td>\n",
       "      <td>1324615</td>\n",
       "      <td>1544518</td>\n",
       "      <td>3056929</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>4.0960</td>\n",
       "      <td>62.75</td>\n",
       "      <td>30.21</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>89.958333</td>\n",
       "      <td>21.3</td>\n",
       "      <td>21.25</td>\n",
       "      <td>20.85</td>\n",
       "      <td>21.135714</td>\n",
       "      <td>20.620000</td>\n",
       "      <td>0.199404</td>\n",
       "      <td>0.339015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.30</td>\n",
       "      <td>94278</td>\n",
       "      <td>1324615</td>\n",
       "      <td>1544518</td>\n",
       "      <td>3056929</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>4.0960</td>\n",
       "      <td>62.75</td>\n",
       "      <td>30.21</td>\n",
       "      <td>4.7</td>\n",
       "      <td>...</td>\n",
       "      <td>90.083333</td>\n",
       "      <td>21.3</td>\n",
       "      <td>21.20</td>\n",
       "      <td>20.85</td>\n",
       "      <td>21.200000</td>\n",
       "      <td>20.660000</td>\n",
       "      <td>0.160728</td>\n",
       "      <td>0.346261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.30</td>\n",
       "      <td>94278</td>\n",
       "      <td>1324615</td>\n",
       "      <td>1544518</td>\n",
       "      <td>3056929</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>4.0960</td>\n",
       "      <td>62.75</td>\n",
       "      <td>30.21</td>\n",
       "      <td>13.2</td>\n",
       "      <td>...</td>\n",
       "      <td>89.125000</td>\n",
       "      <td>21.3</td>\n",
       "      <td>21.30</td>\n",
       "      <td>20.85</td>\n",
       "      <td>21.264286</td>\n",
       "      <td>20.690000</td>\n",
       "      <td>0.047559</td>\n",
       "      <td>0.361606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.35</td>\n",
       "      <td>94278</td>\n",
       "      <td>1324615</td>\n",
       "      <td>1544518</td>\n",
       "      <td>3056929</td>\n",
       "      <td>2207.0</td>\n",
       "      <td>4.0935</td>\n",
       "      <td>62.51</td>\n",
       "      <td>30.44</td>\n",
       "      <td>6.5</td>\n",
       "      <td>...</td>\n",
       "      <td>88.500000</td>\n",
       "      <td>21.3</td>\n",
       "      <td>21.30</td>\n",
       "      <td>21.20</td>\n",
       "      <td>21.285714</td>\n",
       "      <td>20.721667</td>\n",
       "      <td>0.047559</td>\n",
       "      <td>0.376619</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.007075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ffb_1%_oer  import   export  production  end_stock  cpo_futures  \\\n",
       "0       21.30   81477  1680891     1737461    3002871       2200.0   \n",
       "1       21.30   94278  1324615     1544518    3056929       2200.0   \n",
       "2       21.30   94278  1324615     1544518    3056929       2200.0   \n",
       "3       21.30   94278  1324615     1544518    3056929       2200.0   \n",
       "4       21.35   94278  1324615     1544518    3056929       2207.0   \n",
       "\n",
       "   usd_myr_rate  brent_oil_futures  soybean_futures  precipitation  ...  \\\n",
       "0        4.0960              61.89            30.48           47.5  ...   \n",
       "1        4.0960              62.75            30.21            7.0  ...   \n",
       "2        4.0960              62.75            30.21            4.7  ...   \n",
       "3        4.0960              62.75            30.21           13.2  ...   \n",
       "4        4.0935              62.51            30.44            6.5  ...   \n",
       "\n",
       "   avg_humidity  lag_1  lag_3  lag_7  rolling_mean_7  rolling_mean_30  \\\n",
       "0     90.083333   21.2  21.20  20.75       21.071429        20.576667   \n",
       "1     89.958333   21.3  21.25  20.85       21.135714        20.620000   \n",
       "2     90.083333   21.3  21.20  20.85       21.200000        20.660000   \n",
       "3     89.125000   21.3  21.30  20.85       21.264286        20.690000   \n",
       "4     88.500000   21.3  21.30  21.20       21.285714        20.721667   \n",
       "\n",
       "   rolling_std_7  rolling_std_30  pct_change_1  pct_change_7  \n",
       "0       0.209875        0.332113      0.004717      0.026506  \n",
       "1       0.199404        0.339015      0.000000      0.021583  \n",
       "2       0.160728        0.346261      0.000000      0.021583  \n",
       "3       0.047559        0.361606      0.000000      0.021583  \n",
       "4       0.047559        0.376619      0.002347      0.007075  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load cleaned dataset\n",
    "df= pd.read_parquet(\"../data_backend/cleaned_data_2.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecb7da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"ffb_1%_oer\"\n",
    "raw_features = [\"import\", \"export\", \"production\", \"end_stock\", \n",
    "                \"cpo_futures\", \"usd_myr_rate\", \"brent_oil_futures\", \n",
    "                \"soybean_futures\", \"precipitation\", \"avg_temperature\", \"avg_humidity\"]\n",
    "\n",
    "engineered_features = [\"lag_1\",\"lag_3\",\"lag_7\",\"rolling_mean_7\",\n",
    "                       \"rolling_mean_30\",\"rolling_std_7\",\"rolling_std_30\",\n",
    "                       \"pct_change_1\",\"pct_change_7\"]\n",
    "\n",
    "engineered_features_lstm = [\"lag_1\", \"rolling_mean_7\"]\n",
    "\n",
    "X = df[raw_features + engineered_features].values\n",
    "y = df[target_col].values.reshape(-1,1)\n",
    "\n",
    "\n",
    "#Splitting into train-validate-test dataa\n",
    "N = len(df)\n",
    "train_size = int(N * 0.7)   # 70% train\n",
    "val_size   = int(N * 0.2)  # 20% validation\n",
    "test_size  = N - train_size - val_size  # 10% test\n",
    "\n",
    "X_train_raw = X[:train_size]\n",
    "X_val_raw   = X[train_size:train_size+val_size]\n",
    "X_test_raw  = X[train_size+val_size:]\n",
    "\n",
    "y_train_raw = y[:train_size]\n",
    "y_val_raw   = y[train_size:train_size+val_size]\n",
    "y_test_raw  = y[train_size+val_size:]\n",
    "\n",
    "\n",
    "#scale data\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_train = scaler_x.fit_transform(X_train_raw)\n",
    "X_val   = scaler_x.transform(X_val_raw)\n",
    "X_test  = scaler_x.transform(X_test_raw)\n",
    "\n",
    "y_train = scaler_y.fit_transform(y_train_raw)\n",
    "y_val   = scaler_y.transform(y_val_raw)\n",
    "y_test  = scaler_y.transform(y_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35c204a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Flatten last two dimensions: (samples, lookback * features)\\nn_samples, seq_len, n_features = X_train_xgb.shape\\nX_train_xgb = X_train_xgb.reshape(n_samples, seq_len * n_features)\\nX_val_xgb   = X_val_xgb.reshape(X_val_xgb.shape[0], seq_len * n_features)\\nX_test_xgb  = X_test_xgb.reshape(X_test_xgb.shape[0], seq_len * n_features)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_multi_step_sequences(X, y, lookback, horizon):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(lookback, len(X) - horizon + 1):\n",
    "        Xs.append(X[i - lookback:i])\n",
    "        ys.append(y[i:i + horizon].ravel())  # collect next horizon steps\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "forecast_horizon = 14\n",
    "lookback = 90\n",
    "\"\"\"\n",
    "X_train_lstm, y_train_lstm = create_multi_step_sequences(X_train, y_train, lookback, forecast_horizon)\n",
    "X_val_lstm, y_val_lstm     = create_multi_step_sequences(X_val, y_val, lookback, forecast_horizon)\n",
    "X_test_lstm, y_test_lstm   = create_multi_step_sequences(X_test, y_test, lookback, forecast_horizon)\n",
    "\"\"\"\n",
    "# Use same sequence generation logic for fair comparison\n",
    "X_train_xgb, y_train_xgb = create_multi_step_sequences(X_train, y_train, lookback, forecast_horizon)\n",
    "X_val_xgb,   y_val_xgb   = create_multi_step_sequences(X_val, y_val, lookback, forecast_horizon)\n",
    "X_test_xgb,  y_test_xgb  = create_multi_step_sequences(X_test, y_test, lookback, forecast_horizon)\n",
    "\n",
    "\"\"\"\n",
    "# Flatten last two dimensions: (samples, lookback * features)\n",
    "n_samples, seq_len, n_features = X_train_xgb.shape\n",
    "X_train_xgb = X_train_xgb.reshape(n_samples, seq_len * n_features)\n",
    "X_val_xgb   = X_val_xgb.reshape(X_val_xgb.shape[0], seq_len * n_features)\n",
    "X_test_xgb  = X_test_xgb.reshape(X_test_xgb.shape[0], seq_len * n_features)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f4a9639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.20158\tval-rmse:0.03606\n",
      "[1]\ttrain-rmse:0.19174\tval-rmse:0.03425\n",
      "[2]\ttrain-rmse:0.18237\tval-rmse:0.03257\n",
      "[3]\ttrain-rmse:0.17352\tval-rmse:0.03095\n",
      "[4]\ttrain-rmse:0.16511\tval-rmse:0.02951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\ttrain-rmse:0.15708\tval-rmse:0.02803\n",
      "[6]\ttrain-rmse:0.14943\tval-rmse:0.02665\n",
      "[7]\ttrain-rmse:0.14214\tval-rmse:0.02530\n",
      "[8]\ttrain-rmse:0.13527\tval-rmse:0.02405\n",
      "[9]\ttrain-rmse:0.12868\tval-rmse:0.02286\n",
      "[10]\ttrain-rmse:0.12243\tval-rmse:0.02175\n",
      "[11]\ttrain-rmse:0.11651\tval-rmse:0.02070\n",
      "[12]\ttrain-rmse:0.11084\tval-rmse:0.01966\n",
      "[13]\ttrain-rmse:0.10549\tval-rmse:0.01868\n",
      "[14]\ttrain-rmse:0.10044\tval-rmse:0.01780\n",
      "[15]\ttrain-rmse:0.09558\tval-rmse:0.01691\n",
      "[16]\ttrain-rmse:0.09096\tval-rmse:0.01615\n",
      "[17]\ttrain-rmse:0.08656\tval-rmse:0.01537\n",
      "[18]\ttrain-rmse:0.08242\tval-rmse:0.01463\n",
      "[19]\ttrain-rmse:0.07845\tval-rmse:0.01398\n",
      "[20]\ttrain-rmse:0.07467\tval-rmse:0.01326\n",
      "[21]\ttrain-rmse:0.07108\tval-rmse:0.01261\n",
      "[22]\ttrain-rmse:0.06767\tval-rmse:0.01199\n",
      "[23]\ttrain-rmse:0.06445\tval-rmse:0.01145\n",
      "[24]\ttrain-rmse:0.06138\tval-rmse:0.01093\n",
      "[25]\ttrain-rmse:0.05843\tval-rmse:0.01040\n",
      "[26]\ttrain-rmse:0.05566\tval-rmse:0.00982\n",
      "[27]\ttrain-rmse:0.05299\tval-rmse:0.00939\n",
      "[28]\ttrain-rmse:0.05046\tval-rmse:0.00894\n",
      "[29]\ttrain-rmse:0.04808\tval-rmse:0.00856\n",
      "[30]\ttrain-rmse:0.04580\tval-rmse:0.00815\n",
      "[31]\ttrain-rmse:0.04364\tval-rmse:0.00778\n",
      "[32]\ttrain-rmse:0.04159\tval-rmse:0.00743\n",
      "[33]\ttrain-rmse:0.03963\tval-rmse:0.00705\n",
      "[34]\ttrain-rmse:0.03778\tval-rmse:0.00672\n",
      "[35]\ttrain-rmse:0.03602\tval-rmse:0.00644\n",
      "[36]\ttrain-rmse:0.03435\tval-rmse:0.00619\n",
      "[37]\ttrain-rmse:0.03275\tval-rmse:0.00590\n",
      "[38]\ttrain-rmse:0.03125\tval-rmse:0.00564\n",
      "[39]\ttrain-rmse:0.02979\tval-rmse:0.00542\n",
      "[40]\ttrain-rmse:0.02839\tval-rmse:0.00520\n",
      "[41]\ttrain-rmse:0.02707\tval-rmse:0.00497\n",
      "[42]\ttrain-rmse:0.02584\tval-rmse:0.00476\n",
      "[43]\ttrain-rmse:0.02468\tval-rmse:0.00456\n",
      "[44]\ttrain-rmse:0.02355\tval-rmse:0.00437\n",
      "[45]\ttrain-rmse:0.02247\tval-rmse:0.00418\n",
      "[46]\ttrain-rmse:0.02146\tval-rmse:0.00406\n",
      "[47]\ttrain-rmse:0.02050\tval-rmse:0.00390\n",
      "[48]\ttrain-rmse:0.01957\tval-rmse:0.00377\n",
      "[49]\ttrain-rmse:0.01869\tval-rmse:0.00364\n",
      "[50]\ttrain-rmse:0.01786\tval-rmse:0.00353\n",
      "[51]\ttrain-rmse:0.01706\tval-rmse:0.00339\n",
      "[52]\ttrain-rmse:0.01631\tval-rmse:0.00328\n",
      "[53]\ttrain-rmse:0.01561\tval-rmse:0.00316\n",
      "[54]\ttrain-rmse:0.01491\tval-rmse:0.00306\n",
      "[55]\ttrain-rmse:0.01429\tval-rmse:0.00297\n",
      "[56]\ttrain-rmse:0.01366\tval-rmse:0.00290\n",
      "[57]\ttrain-rmse:0.01308\tval-rmse:0.00282\n",
      "[58]\ttrain-rmse:0.01252\tval-rmse:0.00274\n",
      "[59]\ttrain-rmse:0.01198\tval-rmse:0.00268\n",
      "[60]\ttrain-rmse:0.01149\tval-rmse:0.00263\n",
      "[61]\ttrain-rmse:0.01100\tval-rmse:0.00258\n",
      "[62]\ttrain-rmse:0.01055\tval-rmse:0.00254\n",
      "[63]\ttrain-rmse:0.01011\tval-rmse:0.00249\n",
      "[64]\ttrain-rmse:0.00969\tval-rmse:0.00244\n",
      "[65]\ttrain-rmse:0.00928\tval-rmse:0.00240\n",
      "[66]\ttrain-rmse:0.00891\tval-rmse:0.00238\n",
      "[67]\ttrain-rmse:0.00854\tval-rmse:0.00233\n",
      "[68]\ttrain-rmse:0.00819\tval-rmse:0.00229\n",
      "[69]\ttrain-rmse:0.00787\tval-rmse:0.00226\n",
      "[70]\ttrain-rmse:0.00755\tval-rmse:0.00224\n",
      "[71]\ttrain-rmse:0.00726\tval-rmse:0.00221\n",
      "[72]\ttrain-rmse:0.00697\tval-rmse:0.00218\n",
      "[73]\ttrain-rmse:0.00670\tval-rmse:0.00215\n",
      "[74]\ttrain-rmse:0.00644\tval-rmse:0.00214\n",
      "[75]\ttrain-rmse:0.00620\tval-rmse:0.00211\n",
      "[76]\ttrain-rmse:0.00597\tval-rmse:0.00210\n",
      "[77]\ttrain-rmse:0.00575\tval-rmse:0.00208\n",
      "[78]\ttrain-rmse:0.00556\tval-rmse:0.00206\n",
      "[79]\ttrain-rmse:0.00538\tval-rmse:0.00205\n",
      "[80]\ttrain-rmse:0.00518\tval-rmse:0.00204\n",
      "[81]\ttrain-rmse:0.00500\tval-rmse:0.00204\n",
      "[82]\ttrain-rmse:0.00483\tval-rmse:0.00203\n",
      "[83]\ttrain-rmse:0.00467\tval-rmse:0.00202\n",
      "[84]\ttrain-rmse:0.00451\tval-rmse:0.00201\n",
      "[85]\ttrain-rmse:0.00438\tval-rmse:0.00201\n",
      "[86]\ttrain-rmse:0.00425\tval-rmse:0.00200\n",
      "[87]\ttrain-rmse:0.00412\tval-rmse:0.00200\n",
      "[88]\ttrain-rmse:0.00400\tval-rmse:0.00200\n",
      "[89]\ttrain-rmse:0.00388\tval-rmse:0.00199\n",
      "[90]\ttrain-rmse:0.00378\tval-rmse:0.00200\n",
      "[91]\ttrain-rmse:0.00367\tval-rmse:0.00199\n",
      "[92]\ttrain-rmse:0.00357\tval-rmse:0.00199\n",
      "[93]\ttrain-rmse:0.00346\tval-rmse:0.00198\n",
      "[94]\ttrain-rmse:0.00337\tval-rmse:0.00198\n",
      "[95]\ttrain-rmse:0.00328\tval-rmse:0.00197\n",
      "[96]\ttrain-rmse:0.00320\tval-rmse:0.00197\n",
      "[97]\ttrain-rmse:0.00312\tval-rmse:0.00197\n",
      "[98]\ttrain-rmse:0.00304\tval-rmse:0.00196\n",
      "[99]\ttrain-rmse:0.00297\tval-rmse:0.00196\n",
      "[100]\ttrain-rmse:0.00290\tval-rmse:0.00196\n",
      "[101]\ttrain-rmse:0.00283\tval-rmse:0.00195\n",
      "[102]\ttrain-rmse:0.00278\tval-rmse:0.00195\n",
      "[103]\ttrain-rmse:0.00271\tval-rmse:0.00195\n",
      "[104]\ttrain-rmse:0.00266\tval-rmse:0.00195\n",
      "[105]\ttrain-rmse:0.00261\tval-rmse:0.00195\n",
      "[106]\ttrain-rmse:0.00256\tval-rmse:0.00195\n",
      "[107]\ttrain-rmse:0.00251\tval-rmse:0.00195\n",
      "[108]\ttrain-rmse:0.00246\tval-rmse:0.00195\n",
      "[109]\ttrain-rmse:0.00242\tval-rmse:0.00195\n",
      "[110]\ttrain-rmse:0.00237\tval-rmse:0.00194\n",
      "[111]\ttrain-rmse:0.00233\tval-rmse:0.00194\n",
      "[112]\ttrain-rmse:0.00229\tval-rmse:0.00194\n",
      "[113]\ttrain-rmse:0.00226\tval-rmse:0.00194\n",
      "[114]\ttrain-rmse:0.00223\tval-rmse:0.00194\n",
      "[115]\ttrain-rmse:0.00220\tval-rmse:0.00194\n",
      "[116]\ttrain-rmse:0.00217\tval-rmse:0.00194\n",
      "[117]\ttrain-rmse:0.00213\tval-rmse:0.00194\n",
      "[118]\ttrain-rmse:0.00211\tval-rmse:0.00195\n",
      "[119]\ttrain-rmse:0.00208\tval-rmse:0.00195\n",
      "[120]\ttrain-rmse:0.00206\tval-rmse:0.00194\n",
      "[121]\ttrain-rmse:0.00203\tval-rmse:0.00194\n",
      "[122]\ttrain-rmse:0.00200\tval-rmse:0.00194\n",
      "[123]\ttrain-rmse:0.00198\tval-rmse:0.00194\n",
      "[124]\ttrain-rmse:0.00195\tval-rmse:0.00194\n",
      "[125]\ttrain-rmse:0.00194\tval-rmse:0.00194\n",
      "[126]\ttrain-rmse:0.00191\tval-rmse:0.00194\n",
      "[127]\ttrain-rmse:0.00189\tval-rmse:0.00194\n",
      "[128]\ttrain-rmse:0.00187\tval-rmse:0.00194\n",
      "[129]\ttrain-rmse:0.00186\tval-rmse:0.00194\n",
      "[130]\ttrain-rmse:0.00184\tval-rmse:0.00194\n",
      "[131]\ttrain-rmse:0.00183\tval-rmse:0.00194\n",
      "[132]\ttrain-rmse:0.00182\tval-rmse:0.00194\n",
      "[133]\ttrain-rmse:0.00181\tval-rmse:0.00194\n",
      "[134]\ttrain-rmse:0.00179\tval-rmse:0.00194\n",
      "[135]\ttrain-rmse:0.00177\tval-rmse:0.00194\n",
      "[136]\ttrain-rmse:0.00176\tval-rmse:0.00194\n",
      "[137]\ttrain-rmse:0.00175\tval-rmse:0.00194\n",
      "[138]\ttrain-rmse:0.00173\tval-rmse:0.00194\n",
      "[139]\ttrain-rmse:0.00172\tval-rmse:0.00194\n",
      "[140]\ttrain-rmse:0.00171\tval-rmse:0.00194\n",
      "[141]\ttrain-rmse:0.00169\tval-rmse:0.00194\n",
      "[142]\ttrain-rmse:0.00168\tval-rmse:0.00194\n",
      "[143]\ttrain-rmse:0.00167\tval-rmse:0.00194\n",
      "[144]\ttrain-rmse:0.00166\tval-rmse:0.00194\n",
      "[145]\ttrain-rmse:0.00165\tval-rmse:0.00194\n",
      "[146]\ttrain-rmse:0.00164\tval-rmse:0.00194\n",
      "[147]\ttrain-rmse:0.00163\tval-rmse:0.00194\n",
      "[148]\ttrain-rmse:0.00162\tval-rmse:0.00194\n",
      "[149]\ttrain-rmse:0.00161\tval-rmse:0.00194\n",
      "[150]\ttrain-rmse:0.00160\tval-rmse:0.00194\n",
      "[151]\ttrain-rmse:0.00159\tval-rmse:0.00194\n",
      "[152]\ttrain-rmse:0.00159\tval-rmse:0.00194\n",
      "[153]\ttrain-rmse:0.00158\tval-rmse:0.00194\n",
      "[154]\ttrain-rmse:0.00158\tval-rmse:0.00194\n",
      "[155]\ttrain-rmse:0.00157\tval-rmse:0.00194\n",
      "[156]\ttrain-rmse:0.00157\tval-rmse:0.00194\n",
      "[157]\ttrain-rmse:0.00156\tval-rmse:0.00194\n",
      "[158]\ttrain-rmse:0.00155\tval-rmse:0.00194\n",
      "[159]\ttrain-rmse:0.00155\tval-rmse:0.00194\n",
      "[160]\ttrain-rmse:0.00154\tval-rmse:0.00194\n",
      "[161]\ttrain-rmse:0.00154\tval-rmse:0.00194\n",
      "[162]\ttrain-rmse:0.00153\tval-rmse:0.00194\n",
      "[163]\ttrain-rmse:0.00153\tval-rmse:0.00194\n",
      "[164]\ttrain-rmse:0.00152\tval-rmse:0.00193\n",
      "[165]\ttrain-rmse:0.00151\tval-rmse:0.00193\n",
      "[166]\ttrain-rmse:0.00151\tval-rmse:0.00193\n",
      "[167]\ttrain-rmse:0.00149\tval-rmse:0.00194\n",
      "[168]\ttrain-rmse:0.00149\tval-rmse:0.00193\n",
      "[169]\ttrain-rmse:0.00148\tval-rmse:0.00194\n",
      "[170]\ttrain-rmse:0.00147\tval-rmse:0.00194\n",
      "[171]\ttrain-rmse:0.00147\tval-rmse:0.00194\n",
      "[172]\ttrain-rmse:0.00147\tval-rmse:0.00194\n",
      "[173]\ttrain-rmse:0.00146\tval-rmse:0.00194\n",
      "[174]\ttrain-rmse:0.00145\tval-rmse:0.00194\n",
      "[175]\ttrain-rmse:0.00145\tval-rmse:0.00194\n",
      "[176]\ttrain-rmse:0.00144\tval-rmse:0.00194\n",
      "[177]\ttrain-rmse:0.00144\tval-rmse:0.00194\n",
      "[178]\ttrain-rmse:0.00143\tval-rmse:0.00194\n",
      "[179]\ttrain-rmse:0.00143\tval-rmse:0.00194\n",
      "[180]\ttrain-rmse:0.00142\tval-rmse:0.00194\n",
      "[181]\ttrain-rmse:0.00142\tval-rmse:0.00194\n",
      "[182]\ttrain-rmse:0.00142\tval-rmse:0.00194\n",
      "[183]\ttrain-rmse:0.00141\tval-rmse:0.00193\n",
      "[0]\ttrain-rmse:0.20148\tval-rmse:0.03596\n",
      "[1]\ttrain-rmse:0.19166\tval-rmse:0.03421\n",
      "[2]\ttrain-rmse:0.18232\tval-rmse:0.03262\n",
      "[3]\ttrain-rmse:0.17349\tval-rmse:0.03106\n",
      "[4]\ttrain-rmse:0.16511\tval-rmse:0.02953\n",
      "[5]\ttrain-rmse:0.15708\tval-rmse:0.02812\n",
      "[6]\ttrain-rmse:0.14948\tval-rmse:0.02681\n",
      "[7]\ttrain-rmse:0.14222\tval-rmse:0.02549\n",
      "[8]\ttrain-rmse:0.13538\tval-rmse:0.02431\n",
      "[9]\ttrain-rmse:0.12880\tval-rmse:0.02316\n",
      "[10]\ttrain-rmse:0.12258\tval-rmse:0.02207\n",
      "[11]\ttrain-rmse:0.11667\tval-rmse:0.02105\n",
      "[12]\ttrain-rmse:0.11105\tval-rmse:0.02003\n",
      "[13]\ttrain-rmse:0.10572\tval-rmse:0.01908\n",
      "[14]\ttrain-rmse:0.10070\tval-rmse:0.01842\n",
      "[15]\ttrain-rmse:0.09588\tval-rmse:0.01757\n",
      "[16]\ttrain-rmse:0.09128\tval-rmse:0.01681\n",
      "[17]\ttrain-rmse:0.08691\tval-rmse:0.01610\n",
      "[18]\ttrain-rmse:0.08279\tval-rmse:0.01540\n",
      "[19]\ttrain-rmse:0.07884\tval-rmse:0.01477\n",
      "[20]\ttrain-rmse:0.07508\tval-rmse:0.01419\n",
      "[21]\ttrain-rmse:0.07152\tval-rmse:0.01362\n",
      "[22]\ttrain-rmse:0.06814\tval-rmse:0.01305\n",
      "[23]\ttrain-rmse:0.06494\tval-rmse:0.01256\n",
      "[24]\ttrain-rmse:0.06189\tval-rmse:0.01212\n",
      "[25]\ttrain-rmse:0.05898\tval-rmse:0.01176\n",
      "[26]\ttrain-rmse:0.05621\tval-rmse:0.01138\n",
      "[27]\ttrain-rmse:0.05360\tval-rmse:0.01102\n",
      "[28]\ttrain-rmse:0.05110\tval-rmse:0.01062\n",
      "[29]\ttrain-rmse:0.04873\tval-rmse:0.01032\n",
      "[30]\ttrain-rmse:0.04651\tval-rmse:0.00997\n",
      "[31]\ttrain-rmse:0.04434\tval-rmse:0.00969\n",
      "[32]\ttrain-rmse:0.04232\tval-rmse:0.00936\n",
      "[33]\ttrain-rmse:0.04039\tval-rmse:0.00913\n",
      "[34]\ttrain-rmse:0.03853\tval-rmse:0.00888\n",
      "[35]\ttrain-rmse:0.03681\tval-rmse:0.00869\n",
      "[36]\ttrain-rmse:0.03513\tval-rmse:0.00852\n",
      "[37]\ttrain-rmse:0.03357\tval-rmse:0.00831\n",
      "[38]\ttrain-rmse:0.03208\tval-rmse:0.00810\n",
      "[39]\ttrain-rmse:0.03065\tval-rmse:0.00794\n",
      "[40]\ttrain-rmse:0.02930\tval-rmse:0.00778\n",
      "[41]\ttrain-rmse:0.02801\tval-rmse:0.00764\n",
      "[42]\ttrain-rmse:0.02678\tval-rmse:0.00750\n",
      "[43]\ttrain-rmse:0.02563\tval-rmse:0.00737\n",
      "[44]\ttrain-rmse:0.02452\tval-rmse:0.00727\n",
      "[45]\ttrain-rmse:0.02348\tval-rmse:0.00718\n",
      "[46]\ttrain-rmse:0.02249\tval-rmse:0.00713\n",
      "[47]\ttrain-rmse:0.02154\tval-rmse:0.00705\n",
      "[48]\ttrain-rmse:0.02063\tval-rmse:0.00698\n",
      "[49]\ttrain-rmse:0.01978\tval-rmse:0.00692\n",
      "[50]\ttrain-rmse:0.01897\tval-rmse:0.00685\n",
      "[51]\ttrain-rmse:0.01821\tval-rmse:0.00678\n",
      "[52]\ttrain-rmse:0.01748\tval-rmse:0.00674\n",
      "[53]\ttrain-rmse:0.01681\tval-rmse:0.00668\n",
      "[54]\ttrain-rmse:0.01615\tval-rmse:0.00663\n",
      "[55]\ttrain-rmse:0.01555\tval-rmse:0.00662\n",
      "[56]\ttrain-rmse:0.01497\tval-rmse:0.00658\n",
      "[57]\ttrain-rmse:0.01442\tval-rmse:0.00656\n",
      "[58]\ttrain-rmse:0.01389\tval-rmse:0.00653\n",
      "[59]\ttrain-rmse:0.01341\tval-rmse:0.00650\n",
      "[60]\ttrain-rmse:0.01294\tval-rmse:0.00650\n",
      "[61]\ttrain-rmse:0.01250\tval-rmse:0.00650\n",
      "[62]\ttrain-rmse:0.01205\tval-rmse:0.00649\n",
      "[63]\ttrain-rmse:0.01166\tval-rmse:0.00647\n",
      "[64]\ttrain-rmse:0.01129\tval-rmse:0.00645\n",
      "[65]\ttrain-rmse:0.01092\tval-rmse:0.00644\n",
      "[66]\ttrain-rmse:0.01058\tval-rmse:0.00643\n",
      "[67]\ttrain-rmse:0.01027\tval-rmse:0.00643\n",
      "[68]\ttrain-rmse:0.00996\tval-rmse:0.00641\n",
      "[69]\ttrain-rmse:0.00969\tval-rmse:0.00640\n",
      "[70]\ttrain-rmse:0.00941\tval-rmse:0.00639\n",
      "[71]\ttrain-rmse:0.00914\tval-rmse:0.00638\n",
      "[72]\ttrain-rmse:0.00891\tval-rmse:0.00637\n",
      "[73]\ttrain-rmse:0.00867\tval-rmse:0.00636\n",
      "[74]\ttrain-rmse:0.00846\tval-rmse:0.00636\n",
      "[75]\ttrain-rmse:0.00826\tval-rmse:0.00635\n",
      "[76]\ttrain-rmse:0.00809\tval-rmse:0.00635\n",
      "[77]\ttrain-rmse:0.00790\tval-rmse:0.00634\n",
      "[78]\ttrain-rmse:0.00774\tval-rmse:0.00634\n",
      "[79]\ttrain-rmse:0.00758\tval-rmse:0.00635\n",
      "[80]\ttrain-rmse:0.00743\tval-rmse:0.00635\n",
      "[81]\ttrain-rmse:0.00728\tval-rmse:0.00635\n",
      "[82]\ttrain-rmse:0.00714\tval-rmse:0.00635\n",
      "[83]\ttrain-rmse:0.00701\tval-rmse:0.00635\n",
      "[84]\ttrain-rmse:0.00690\tval-rmse:0.00635\n",
      "[85]\ttrain-rmse:0.00679\tval-rmse:0.00634\n",
      "[86]\ttrain-rmse:0.00668\tval-rmse:0.00635\n",
      "[87]\ttrain-rmse:0.00658\tval-rmse:0.00636\n",
      "[88]\ttrain-rmse:0.00648\tval-rmse:0.00637\n",
      "[89]\ttrain-rmse:0.00639\tval-rmse:0.00638\n",
      "[90]\ttrain-rmse:0.00629\tval-rmse:0.00638\n",
      "[91]\ttrain-rmse:0.00622\tval-rmse:0.00637\n",
      "[92]\ttrain-rmse:0.00614\tval-rmse:0.00638\n",
      "[93]\ttrain-rmse:0.00606\tval-rmse:0.00638\n",
      "[94]\ttrain-rmse:0.00599\tval-rmse:0.00638\n",
      "[95]\ttrain-rmse:0.00592\tval-rmse:0.00638\n",
      "[96]\ttrain-rmse:0.00586\tval-rmse:0.00638\n",
      "[0]\ttrain-rmse:0.20136\tval-rmse:0.03596\n",
      "[1]\ttrain-rmse:0.19156\tval-rmse:0.03424\n",
      "[2]\ttrain-rmse:0.18225\tval-rmse:0.03263\n",
      "[3]\ttrain-rmse:0.17344\tval-rmse:0.03111\n",
      "[4]\ttrain-rmse:0.16509\tval-rmse:0.02958\n",
      "[5]\ttrain-rmse:0.15708\tval-rmse:0.02817\n",
      "[6]\ttrain-rmse:0.14952\tval-rmse:0.02686\n",
      "[7]\ttrain-rmse:0.14227\tval-rmse:0.02567\n",
      "[8]\ttrain-rmse:0.13546\tval-rmse:0.02451\n",
      "[9]\ttrain-rmse:0.12889\tval-rmse:0.02345\n",
      "[10]\ttrain-rmse:0.12271\tval-rmse:0.02245\n",
      "[11]\ttrain-rmse:0.11682\tval-rmse:0.02148\n",
      "[12]\ttrain-rmse:0.11123\tval-rmse:0.02053\n",
      "[13]\ttrain-rmse:0.10592\tval-rmse:0.01969\n",
      "[14]\ttrain-rmse:0.10092\tval-rmse:0.01899\n",
      "[15]\ttrain-rmse:0.09613\tval-rmse:0.01821\n",
      "[16]\ttrain-rmse:0.09156\tval-rmse:0.01753\n",
      "[17]\ttrain-rmse:0.08721\tval-rmse:0.01698\n",
      "[18]\ttrain-rmse:0.08312\tval-rmse:0.01639\n",
      "[19]\ttrain-rmse:0.07920\tval-rmse:0.01578\n",
      "[20]\ttrain-rmse:0.07546\tval-rmse:0.01536\n",
      "[21]\ttrain-rmse:0.07192\tval-rmse:0.01483\n",
      "[22]\ttrain-rmse:0.06858\tval-rmse:0.01434\n",
      "[23]\ttrain-rmse:0.06537\tval-rmse:0.01394\n",
      "[24]\ttrain-rmse:0.06236\tval-rmse:0.01363\n",
      "[25]\ttrain-rmse:0.05949\tval-rmse:0.01332\n",
      "[26]\ttrain-rmse:0.05675\tval-rmse:0.01300\n",
      "[27]\ttrain-rmse:0.05413\tval-rmse:0.01274\n",
      "[28]\ttrain-rmse:0.05166\tval-rmse:0.01245\n",
      "[29]\ttrain-rmse:0.04930\tval-rmse:0.01222\n",
      "[30]\ttrain-rmse:0.04712\tval-rmse:0.01195\n",
      "[31]\ttrain-rmse:0.04499\tval-rmse:0.01173\n",
      "[32]\ttrain-rmse:0.04299\tval-rmse:0.01147\n",
      "[33]\ttrain-rmse:0.04108\tval-rmse:0.01131\n",
      "[34]\ttrain-rmse:0.03926\tval-rmse:0.01114\n",
      "[35]\ttrain-rmse:0.03757\tval-rmse:0.01098\n",
      "[36]\ttrain-rmse:0.03590\tval-rmse:0.01086\n",
      "[37]\ttrain-rmse:0.03435\tval-rmse:0.01069\n",
      "[38]\ttrain-rmse:0.03288\tval-rmse:0.01057\n",
      "[39]\ttrain-rmse:0.03147\tval-rmse:0.01042\n",
      "[40]\ttrain-rmse:0.03013\tval-rmse:0.01033\n",
      "[41]\ttrain-rmse:0.02887\tval-rmse:0.01023\n",
      "[42]\ttrain-rmse:0.02767\tval-rmse:0.01014\n",
      "[43]\ttrain-rmse:0.02654\tval-rmse:0.01004\n",
      "[44]\ttrain-rmse:0.02547\tval-rmse:0.00998\n",
      "[45]\ttrain-rmse:0.02447\tval-rmse:0.00994\n",
      "[46]\ttrain-rmse:0.02349\tval-rmse:0.00993\n",
      "[47]\ttrain-rmse:0.02255\tval-rmse:0.00986\n",
      "[48]\ttrain-rmse:0.02169\tval-rmse:0.00982\n",
      "[49]\ttrain-rmse:0.02089\tval-rmse:0.00976\n",
      "[50]\ttrain-rmse:0.02011\tval-rmse:0.00974\n",
      "[51]\ttrain-rmse:0.01936\tval-rmse:0.00969\n",
      "[52]\ttrain-rmse:0.01867\tval-rmse:0.00967\n",
      "[53]\ttrain-rmse:0.01799\tval-rmse:0.00964\n",
      "[54]\ttrain-rmse:0.01735\tval-rmse:0.00958\n",
      "[55]\ttrain-rmse:0.01676\tval-rmse:0.00954\n",
      "[56]\ttrain-rmse:0.01621\tval-rmse:0.00952\n",
      "[57]\ttrain-rmse:0.01570\tval-rmse:0.00951\n",
      "[58]\ttrain-rmse:0.01520\tval-rmse:0.00950\n",
      "[59]\ttrain-rmse:0.01473\tval-rmse:0.00948\n",
      "[60]\ttrain-rmse:0.01428\tval-rmse:0.00949\n",
      "[61]\ttrain-rmse:0.01384\tval-rmse:0.00950\n",
      "[62]\ttrain-rmse:0.01344\tval-rmse:0.00949\n",
      "[63]\ttrain-rmse:0.01307\tval-rmse:0.00948\n",
      "[64]\ttrain-rmse:0.01270\tval-rmse:0.00948\n",
      "[65]\ttrain-rmse:0.01236\tval-rmse:0.00948\n",
      "[66]\ttrain-rmse:0.01204\tval-rmse:0.00949\n",
      "[67]\ttrain-rmse:0.01175\tval-rmse:0.00950\n",
      "[68]\ttrain-rmse:0.01147\tval-rmse:0.00949\n",
      "[69]\ttrain-rmse:0.01121\tval-rmse:0.00949\n",
      "[70]\ttrain-rmse:0.01095\tval-rmse:0.00950\n",
      "[71]\ttrain-rmse:0.01072\tval-rmse:0.00951\n",
      "[72]\ttrain-rmse:0.01050\tval-rmse:0.00953\n",
      "[73]\ttrain-rmse:0.01030\tval-rmse:0.00954\n",
      "[74]\ttrain-rmse:0.01011\tval-rmse:0.00954\n",
      "[75]\ttrain-rmse:0.00993\tval-rmse:0.00954\n",
      "[76]\ttrain-rmse:0.00976\tval-rmse:0.00955\n",
      "[77]\ttrain-rmse:0.00960\tval-rmse:0.00955\n",
      "[78]\ttrain-rmse:0.00945\tval-rmse:0.00956\n",
      "[0]\ttrain-rmse:0.20124\tval-rmse:0.03583\n",
      "[1]\ttrain-rmse:0.19146\tval-rmse:0.03416\n",
      "[2]\ttrain-rmse:0.18218\tval-rmse:0.03259\n",
      "[3]\ttrain-rmse:0.17339\tval-rmse:0.03118\n",
      "[4]\ttrain-rmse:0.16506\tval-rmse:0.02975\n",
      "[5]\ttrain-rmse:0.15706\tval-rmse:0.02838\n",
      "[6]\ttrain-rmse:0.14954\tval-rmse:0.02715\n",
      "[7]\ttrain-rmse:0.14232\tval-rmse:0.02599\n",
      "[8]\ttrain-rmse:0.13552\tval-rmse:0.02483\n",
      "[9]\ttrain-rmse:0.12898\tval-rmse:0.02385\n",
      "[10]\ttrain-rmse:0.12282\tval-rmse:0.02293\n",
      "[11]\ttrain-rmse:0.11694\tval-rmse:0.02195\n",
      "[12]\ttrain-rmse:0.11139\tval-rmse:0.02108\n",
      "[13]\ttrain-rmse:0.10610\tval-rmse:0.02027\n",
      "[14]\ttrain-rmse:0.10113\tval-rmse:0.01954\n",
      "[15]\ttrain-rmse:0.09637\tval-rmse:0.01888\n",
      "[16]\ttrain-rmse:0.09183\tval-rmse:0.01830\n",
      "[17]\ttrain-rmse:0.08748\tval-rmse:0.01783\n",
      "[18]\ttrain-rmse:0.08338\tval-rmse:0.01726\n",
      "[19]\ttrain-rmse:0.07947\tval-rmse:0.01671\n",
      "[20]\ttrain-rmse:0.07574\tval-rmse:0.01623\n",
      "[21]\ttrain-rmse:0.07220\tval-rmse:0.01581\n",
      "[22]\ttrain-rmse:0.06887\tval-rmse:0.01531\n",
      "[23]\ttrain-rmse:0.06569\tval-rmse:0.01498\n",
      "[24]\ttrain-rmse:0.06269\tval-rmse:0.01466\n",
      "[25]\ttrain-rmse:0.05983\tval-rmse:0.01439\n",
      "[26]\ttrain-rmse:0.05708\tval-rmse:0.01411\n",
      "[27]\ttrain-rmse:0.05449\tval-rmse:0.01384\n",
      "[28]\ttrain-rmse:0.05204\tval-rmse:0.01354\n",
      "[29]\ttrain-rmse:0.04969\tval-rmse:0.01340\n",
      "[30]\ttrain-rmse:0.04750\tval-rmse:0.01321\n",
      "[31]\ttrain-rmse:0.04538\tval-rmse:0.01301\n",
      "[32]\ttrain-rmse:0.04337\tval-rmse:0.01286\n",
      "[33]\ttrain-rmse:0.04147\tval-rmse:0.01282\n",
      "[34]\ttrain-rmse:0.03965\tval-rmse:0.01274\n",
      "[35]\ttrain-rmse:0.03795\tval-rmse:0.01265\n",
      "[36]\ttrain-rmse:0.03633\tval-rmse:0.01255\n",
      "[37]\ttrain-rmse:0.03478\tval-rmse:0.01236\n",
      "[38]\ttrain-rmse:0.03332\tval-rmse:0.01228\n",
      "[39]\ttrain-rmse:0.03193\tval-rmse:0.01218\n",
      "[40]\ttrain-rmse:0.03061\tval-rmse:0.01212\n",
      "[41]\ttrain-rmse:0.02937\tval-rmse:0.01209\n",
      "[42]\ttrain-rmse:0.02821\tval-rmse:0.01206\n",
      "[43]\ttrain-rmse:0.02710\tval-rmse:0.01196\n",
      "[44]\ttrain-rmse:0.02605\tval-rmse:0.01194\n",
      "[45]\ttrain-rmse:0.02505\tval-rmse:0.01188\n",
      "[46]\ttrain-rmse:0.02406\tval-rmse:0.01189\n",
      "[47]\ttrain-rmse:0.02314\tval-rmse:0.01184\n",
      "[48]\ttrain-rmse:0.02229\tval-rmse:0.01184\n",
      "[49]\ttrain-rmse:0.02148\tval-rmse:0.01181\n",
      "[50]\ttrain-rmse:0.02071\tval-rmse:0.01178\n",
      "[51]\ttrain-rmse:0.01997\tval-rmse:0.01179\n",
      "[52]\ttrain-rmse:0.01927\tval-rmse:0.01179\n",
      "[53]\ttrain-rmse:0.01862\tval-rmse:0.01177\n",
      "[54]\ttrain-rmse:0.01800\tval-rmse:0.01172\n",
      "[55]\ttrain-rmse:0.01741\tval-rmse:0.01171\n",
      "[56]\ttrain-rmse:0.01688\tval-rmse:0.01172\n",
      "[57]\ttrain-rmse:0.01636\tval-rmse:0.01173\n",
      "[58]\ttrain-rmse:0.01586\tval-rmse:0.01175\n",
      "[59]\ttrain-rmse:0.01538\tval-rmse:0.01177\n",
      "[60]\ttrain-rmse:0.01495\tval-rmse:0.01177\n",
      "[61]\ttrain-rmse:0.01452\tval-rmse:0.01179\n",
      "[62]\ttrain-rmse:0.01414\tval-rmse:0.01181\n",
      "[63]\ttrain-rmse:0.01378\tval-rmse:0.01182\n",
      "[64]\ttrain-rmse:0.01343\tval-rmse:0.01182\n",
      "[65]\ttrain-rmse:0.01311\tval-rmse:0.01186\n",
      "[66]\ttrain-rmse:0.01280\tval-rmse:0.01188\n",
      "[67]\ttrain-rmse:0.01250\tval-rmse:0.01191\n",
      "[68]\ttrain-rmse:0.01222\tval-rmse:0.01195\n",
      "[69]\ttrain-rmse:0.01195\tval-rmse:0.01195\n",
      "[70]\ttrain-rmse:0.01171\tval-rmse:0.01197\n",
      "[71]\ttrain-rmse:0.01147\tval-rmse:0.01201\n",
      "[72]\ttrain-rmse:0.01123\tval-rmse:0.01203\n",
      "[73]\ttrain-rmse:0.01102\tval-rmse:0.01203\n",
      "[74]\ttrain-rmse:0.01083\tval-rmse:0.01203\n",
      "[75]\ttrain-rmse:0.01064\tval-rmse:0.01207\n",
      "[0]\ttrain-rmse:0.20117\tval-rmse:0.03590\n",
      "[1]\ttrain-rmse:0.19142\tval-rmse:0.03430\n",
      "[2]\ttrain-rmse:0.18215\tval-rmse:0.03275\n",
      "[3]\ttrain-rmse:0.17338\tval-rmse:0.03134\n",
      "[4]\ttrain-rmse:0.16508\tval-rmse:0.02997\n",
      "[5]\ttrain-rmse:0.15711\tval-rmse:0.02867\n",
      "[6]\ttrain-rmse:0.14962\tval-rmse:0.02742\n",
      "[7]\ttrain-rmse:0.14243\tval-rmse:0.02624\n",
      "[8]\ttrain-rmse:0.13565\tval-rmse:0.02513\n",
      "[9]\ttrain-rmse:0.12915\tval-rmse:0.02430\n",
      "[10]\ttrain-rmse:0.12298\tval-rmse:0.02336\n",
      "[11]\ttrain-rmse:0.11711\tval-rmse:0.02247\n",
      "[12]\ttrain-rmse:0.11158\tval-rmse:0.02167\n",
      "[13]\ttrain-rmse:0.10630\tval-rmse:0.02089\n",
      "[14]\ttrain-rmse:0.10135\tval-rmse:0.02041\n",
      "[15]\ttrain-rmse:0.09660\tval-rmse:0.01968\n",
      "[16]\ttrain-rmse:0.09204\tval-rmse:0.01906\n",
      "[17]\ttrain-rmse:0.08770\tval-rmse:0.01864\n",
      "[18]\ttrain-rmse:0.08363\tval-rmse:0.01827\n",
      "[19]\ttrain-rmse:0.07975\tval-rmse:0.01775\n",
      "[20]\ttrain-rmse:0.07603\tval-rmse:0.01731\n",
      "[21]\ttrain-rmse:0.07252\tval-rmse:0.01696\n",
      "[22]\ttrain-rmse:0.06920\tval-rmse:0.01651\n",
      "[23]\ttrain-rmse:0.06601\tval-rmse:0.01620\n",
      "[24]\ttrain-rmse:0.06301\tval-rmse:0.01593\n",
      "[25]\ttrain-rmse:0.06016\tval-rmse:0.01579\n",
      "[26]\ttrain-rmse:0.05741\tval-rmse:0.01552\n",
      "[27]\ttrain-rmse:0.05484\tval-rmse:0.01538\n",
      "[28]\ttrain-rmse:0.05239\tval-rmse:0.01516\n",
      "[29]\ttrain-rmse:0.05005\tval-rmse:0.01510\n",
      "[30]\ttrain-rmse:0.04787\tval-rmse:0.01491\n",
      "[31]\ttrain-rmse:0.04577\tval-rmse:0.01476\n",
      "[32]\ttrain-rmse:0.04376\tval-rmse:0.01458\n",
      "[33]\ttrain-rmse:0.04188\tval-rmse:0.01444\n",
      "[34]\ttrain-rmse:0.04008\tval-rmse:0.01435\n",
      "[35]\ttrain-rmse:0.03839\tval-rmse:0.01432\n",
      "[36]\ttrain-rmse:0.03678\tval-rmse:0.01423\n",
      "[37]\ttrain-rmse:0.03524\tval-rmse:0.01413\n",
      "[38]\ttrain-rmse:0.03381\tval-rmse:0.01408\n",
      "[39]\ttrain-rmse:0.03245\tval-rmse:0.01392\n",
      "[40]\ttrain-rmse:0.03114\tval-rmse:0.01382\n",
      "[41]\ttrain-rmse:0.02993\tval-rmse:0.01379\n",
      "[42]\ttrain-rmse:0.02877\tval-rmse:0.01376\n",
      "[43]\ttrain-rmse:0.02765\tval-rmse:0.01370\n",
      "[44]\ttrain-rmse:0.02658\tval-rmse:0.01366\n",
      "[45]\ttrain-rmse:0.02560\tval-rmse:0.01363\n",
      "[46]\ttrain-rmse:0.02465\tval-rmse:0.01360\n",
      "[47]\ttrain-rmse:0.02374\tval-rmse:0.01357\n",
      "[48]\ttrain-rmse:0.02292\tval-rmse:0.01361\n",
      "[49]\ttrain-rmse:0.02212\tval-rmse:0.01359\n",
      "[50]\ttrain-rmse:0.02134\tval-rmse:0.01358\n",
      "[51]\ttrain-rmse:0.02063\tval-rmse:0.01354\n",
      "[52]\ttrain-rmse:0.01995\tval-rmse:0.01355\n",
      "[53]\ttrain-rmse:0.01931\tval-rmse:0.01351\n",
      "[54]\ttrain-rmse:0.01871\tval-rmse:0.01349\n",
      "[55]\ttrain-rmse:0.01813\tval-rmse:0.01346\n",
      "[56]\ttrain-rmse:0.01762\tval-rmse:0.01349\n",
      "[57]\ttrain-rmse:0.01710\tval-rmse:0.01349\n",
      "[58]\ttrain-rmse:0.01662\tval-rmse:0.01354\n",
      "[59]\ttrain-rmse:0.01617\tval-rmse:0.01355\n",
      "[60]\ttrain-rmse:0.01574\tval-rmse:0.01357\n",
      "[61]\ttrain-rmse:0.01534\tval-rmse:0.01357\n",
      "[62]\ttrain-rmse:0.01496\tval-rmse:0.01362\n",
      "[63]\ttrain-rmse:0.01461\tval-rmse:0.01360\n",
      "[64]\ttrain-rmse:0.01426\tval-rmse:0.01364\n",
      "[65]\ttrain-rmse:0.01393\tval-rmse:0.01365\n",
      "[66]\ttrain-rmse:0.01361\tval-rmse:0.01363\n",
      "[67]\ttrain-rmse:0.01330\tval-rmse:0.01364\n",
      "[68]\ttrain-rmse:0.01301\tval-rmse:0.01366\n",
      "[69]\ttrain-rmse:0.01275\tval-rmse:0.01369\n",
      "[70]\ttrain-rmse:0.01252\tval-rmse:0.01370\n",
      "[71]\ttrain-rmse:0.01229\tval-rmse:0.01373\n",
      "[72]\ttrain-rmse:0.01205\tval-rmse:0.01374\n",
      "[73]\ttrain-rmse:0.01184\tval-rmse:0.01374\n",
      "[74]\ttrain-rmse:0.01165\tval-rmse:0.01376\n",
      "[0]\ttrain-rmse:0.20109\tval-rmse:0.03595\n",
      "[1]\ttrain-rmse:0.19136\tval-rmse:0.03441\n",
      "[2]\ttrain-rmse:0.18212\tval-rmse:0.03302\n",
      "[3]\ttrain-rmse:0.17340\tval-rmse:0.03173\n",
      "[4]\ttrain-rmse:0.16514\tval-rmse:0.03065\n",
      "[5]\ttrain-rmse:0.15720\tval-rmse:0.02940\n",
      "[6]\ttrain-rmse:0.14974\tval-rmse:0.02839\n",
      "[7]\ttrain-rmse:0.14258\tval-rmse:0.02731\n",
      "[8]\ttrain-rmse:0.13585\tval-rmse:0.02626\n",
      "[9]\ttrain-rmse:0.12937\tval-rmse:0.02544\n",
      "[10]\ttrain-rmse:0.12321\tval-rmse:0.02466\n",
      "[11]\ttrain-rmse:0.11738\tval-rmse:0.02387\n",
      "[12]\ttrain-rmse:0.11185\tval-rmse:0.02308\n",
      "[13]\ttrain-rmse:0.10657\tval-rmse:0.02239\n",
      "[14]\ttrain-rmse:0.10156\tval-rmse:0.02183\n",
      "[15]\ttrain-rmse:0.09684\tval-rmse:0.02125\n",
      "[16]\ttrain-rmse:0.09229\tval-rmse:0.02071\n",
      "[17]\ttrain-rmse:0.08799\tval-rmse:0.02035\n",
      "[18]\ttrain-rmse:0.08393\tval-rmse:0.01998\n",
      "[19]\ttrain-rmse:0.08007\tval-rmse:0.01951\n",
      "[20]\ttrain-rmse:0.07636\tval-rmse:0.01914\n",
      "[21]\ttrain-rmse:0.07285\tval-rmse:0.01877\n",
      "[22]\ttrain-rmse:0.06955\tval-rmse:0.01826\n",
      "[23]\ttrain-rmse:0.06639\tval-rmse:0.01802\n",
      "[24]\ttrain-rmse:0.06341\tval-rmse:0.01776\n",
      "[25]\ttrain-rmse:0.06054\tval-rmse:0.01765\n",
      "[26]\ttrain-rmse:0.05778\tval-rmse:0.01739\n",
      "[27]\ttrain-rmse:0.05519\tval-rmse:0.01726\n",
      "[28]\ttrain-rmse:0.05275\tval-rmse:0.01704\n",
      "[29]\ttrain-rmse:0.05040\tval-rmse:0.01695\n",
      "[30]\ttrain-rmse:0.04822\tval-rmse:0.01679\n",
      "[31]\ttrain-rmse:0.04612\tval-rmse:0.01662\n",
      "[32]\ttrain-rmse:0.04414\tval-rmse:0.01649\n",
      "[33]\ttrain-rmse:0.04225\tval-rmse:0.01647\n",
      "[34]\ttrain-rmse:0.04045\tval-rmse:0.01642\n",
      "[35]\ttrain-rmse:0.03879\tval-rmse:0.01635\n",
      "[36]\ttrain-rmse:0.03720\tval-rmse:0.01627\n",
      "[37]\ttrain-rmse:0.03570\tval-rmse:0.01614\n",
      "[38]\ttrain-rmse:0.03428\tval-rmse:0.01601\n",
      "[39]\ttrain-rmse:0.03295\tval-rmse:0.01596\n",
      "[40]\ttrain-rmse:0.03165\tval-rmse:0.01585\n",
      "[41]\ttrain-rmse:0.03044\tval-rmse:0.01586\n",
      "[42]\ttrain-rmse:0.02929\tval-rmse:0.01586\n",
      "[43]\ttrain-rmse:0.02820\tval-rmse:0.01579\n",
      "[44]\ttrain-rmse:0.02717\tval-rmse:0.01572\n",
      "[45]\ttrain-rmse:0.02622\tval-rmse:0.01571\n",
      "[46]\ttrain-rmse:0.02526\tval-rmse:0.01577\n",
      "[47]\ttrain-rmse:0.02439\tval-rmse:0.01570\n",
      "[48]\ttrain-rmse:0.02358\tval-rmse:0.01572\n",
      "[49]\ttrain-rmse:0.02281\tval-rmse:0.01568\n",
      "[50]\ttrain-rmse:0.02204\tval-rmse:0.01568\n",
      "[51]\ttrain-rmse:0.02133\tval-rmse:0.01565\n",
      "[52]\ttrain-rmse:0.02066\tval-rmse:0.01570\n",
      "[53]\ttrain-rmse:0.02003\tval-rmse:0.01567\n",
      "[54]\ttrain-rmse:0.01940\tval-rmse:0.01566\n",
      "[55]\ttrain-rmse:0.01882\tval-rmse:0.01565\n",
      "[56]\ttrain-rmse:0.01830\tval-rmse:0.01566\n",
      "[57]\ttrain-rmse:0.01779\tval-rmse:0.01566\n",
      "[58]\ttrain-rmse:0.01731\tval-rmse:0.01567\n",
      "[59]\ttrain-rmse:0.01688\tval-rmse:0.01565\n",
      "[60]\ttrain-rmse:0.01645\tval-rmse:0.01566\n",
      "[61]\ttrain-rmse:0.01606\tval-rmse:0.01567\n",
      "[62]\ttrain-rmse:0.01567\tval-rmse:0.01570\n",
      "[63]\ttrain-rmse:0.01533\tval-rmse:0.01568\n",
      "[64]\ttrain-rmse:0.01500\tval-rmse:0.01572\n",
      "[65]\ttrain-rmse:0.01469\tval-rmse:0.01572\n",
      "[66]\ttrain-rmse:0.01435\tval-rmse:0.01578\n",
      "[67]\ttrain-rmse:0.01404\tval-rmse:0.01581\n",
      "[68]\ttrain-rmse:0.01377\tval-rmse:0.01584\n",
      "[69]\ttrain-rmse:0.01351\tval-rmse:0.01585\n",
      "[70]\ttrain-rmse:0.01327\tval-rmse:0.01586\n",
      "[71]\ttrain-rmse:0.01303\tval-rmse:0.01591\n",
      "[0]\ttrain-rmse:0.20098\tval-rmse:0.03612\n",
      "[1]\ttrain-rmse:0.19127\tval-rmse:0.03469\n",
      "[2]\ttrain-rmse:0.18207\tval-rmse:0.03337\n",
      "[3]\ttrain-rmse:0.17337\tval-rmse:0.03214\n",
      "[4]\ttrain-rmse:0.16514\tval-rmse:0.03109\n",
      "[5]\ttrain-rmse:0.15723\tval-rmse:0.02994\n",
      "[6]\ttrain-rmse:0.14980\tval-rmse:0.02892\n",
      "[7]\ttrain-rmse:0.14269\tval-rmse:0.02786\n",
      "[8]\ttrain-rmse:0.13595\tval-rmse:0.02695\n",
      "[9]\ttrain-rmse:0.12950\tval-rmse:0.02605\n",
      "[10]\ttrain-rmse:0.12337\tval-rmse:0.02525\n",
      "[11]\ttrain-rmse:0.11760\tval-rmse:0.02452\n",
      "[12]\ttrain-rmse:0.11208\tval-rmse:0.02377\n",
      "[13]\ttrain-rmse:0.10679\tval-rmse:0.02316\n",
      "[14]\ttrain-rmse:0.10184\tval-rmse:0.02264\n",
      "[15]\ttrain-rmse:0.09708\tval-rmse:0.02219\n",
      "[16]\ttrain-rmse:0.09254\tval-rmse:0.02171\n",
      "[17]\ttrain-rmse:0.08825\tval-rmse:0.02126\n",
      "[18]\ttrain-rmse:0.08417\tval-rmse:0.02097\n",
      "[19]\ttrain-rmse:0.08034\tval-rmse:0.02064\n",
      "[20]\ttrain-rmse:0.07664\tval-rmse:0.02036\n",
      "[21]\ttrain-rmse:0.07313\tval-rmse:0.02004\n",
      "[22]\ttrain-rmse:0.06983\tval-rmse:0.01975\n",
      "[23]\ttrain-rmse:0.06665\tval-rmse:0.01953\n",
      "[24]\ttrain-rmse:0.06368\tval-rmse:0.01932\n",
      "[25]\ttrain-rmse:0.06084\tval-rmse:0.01921\n",
      "[26]\ttrain-rmse:0.05809\tval-rmse:0.01894\n",
      "[27]\ttrain-rmse:0.05551\tval-rmse:0.01888\n",
      "[28]\ttrain-rmse:0.05306\tval-rmse:0.01859\n",
      "[29]\ttrain-rmse:0.05071\tval-rmse:0.01847\n",
      "[30]\ttrain-rmse:0.04857\tval-rmse:0.01829\n",
      "[31]\ttrain-rmse:0.04649\tval-rmse:0.01816\n",
      "[32]\ttrain-rmse:0.04451\tval-rmse:0.01801\n",
      "[33]\ttrain-rmse:0.04262\tval-rmse:0.01800\n",
      "[34]\ttrain-rmse:0.04083\tval-rmse:0.01792\n",
      "[35]\ttrain-rmse:0.03918\tval-rmse:0.01780\n",
      "[36]\ttrain-rmse:0.03760\tval-rmse:0.01776\n",
      "[37]\ttrain-rmse:0.03610\tval-rmse:0.01775\n",
      "[38]\ttrain-rmse:0.03469\tval-rmse:0.01766\n",
      "[39]\ttrain-rmse:0.03334\tval-rmse:0.01762\n",
      "[40]\ttrain-rmse:0.03205\tval-rmse:0.01759\n",
      "[41]\ttrain-rmse:0.03085\tval-rmse:0.01753\n",
      "[42]\ttrain-rmse:0.02970\tval-rmse:0.01746\n",
      "[43]\ttrain-rmse:0.02862\tval-rmse:0.01744\n",
      "[44]\ttrain-rmse:0.02760\tval-rmse:0.01743\n",
      "[45]\ttrain-rmse:0.02665\tval-rmse:0.01752\n",
      "[46]\ttrain-rmse:0.02572\tval-rmse:0.01757\n",
      "[47]\ttrain-rmse:0.02482\tval-rmse:0.01757\n",
      "[48]\ttrain-rmse:0.02397\tval-rmse:0.01754\n",
      "[49]\ttrain-rmse:0.02320\tval-rmse:0.01758\n",
      "[50]\ttrain-rmse:0.02246\tval-rmse:0.01753\n",
      "[51]\ttrain-rmse:0.02175\tval-rmse:0.01749\n",
      "[52]\ttrain-rmse:0.02108\tval-rmse:0.01756\n",
      "[53]\ttrain-rmse:0.02044\tval-rmse:0.01758\n",
      "[54]\ttrain-rmse:0.01985\tval-rmse:0.01759\n",
      "[55]\ttrain-rmse:0.01929\tval-rmse:0.01755\n",
      "[56]\ttrain-rmse:0.01875\tval-rmse:0.01755\n",
      "[57]\ttrain-rmse:0.01826\tval-rmse:0.01757\n",
      "[58]\ttrain-rmse:0.01779\tval-rmse:0.01761\n",
      "[59]\ttrain-rmse:0.01737\tval-rmse:0.01763\n",
      "[60]\ttrain-rmse:0.01695\tval-rmse:0.01775\n",
      "[61]\ttrain-rmse:0.01654\tval-rmse:0.01783\n",
      "[62]\ttrain-rmse:0.01619\tval-rmse:0.01789\n",
      "[63]\ttrain-rmse:0.01584\tval-rmse:0.01787\n",
      "[64]\ttrain-rmse:0.01552\tval-rmse:0.01798\n",
      "[0]\ttrain-rmse:0.20088\tval-rmse:0.03615\n",
      "[1]\ttrain-rmse:0.19122\tval-rmse:0.03468\n",
      "[2]\ttrain-rmse:0.18202\tval-rmse:0.03331\n",
      "[3]\ttrain-rmse:0.17335\tval-rmse:0.03211\n",
      "[4]\ttrain-rmse:0.16514\tval-rmse:0.03107\n",
      "[5]\ttrain-rmse:0.15725\tval-rmse:0.02993\n",
      "[6]\ttrain-rmse:0.14991\tval-rmse:0.02897\n",
      "[7]\ttrain-rmse:0.14280\tval-rmse:0.02799\n",
      "[8]\ttrain-rmse:0.13605\tval-rmse:0.02711\n",
      "[9]\ttrain-rmse:0.12962\tval-rmse:0.02624\n",
      "[10]\ttrain-rmse:0.12350\tval-rmse:0.02561\n",
      "[11]\ttrain-rmse:0.11776\tval-rmse:0.02492\n",
      "[12]\ttrain-rmse:0.11221\tval-rmse:0.02417\n",
      "[13]\ttrain-rmse:0.10695\tval-rmse:0.02369\n",
      "[14]\ttrain-rmse:0.10200\tval-rmse:0.02311\n",
      "[15]\ttrain-rmse:0.09726\tval-rmse:0.02261\n",
      "[16]\ttrain-rmse:0.09272\tval-rmse:0.02208\n",
      "[17]\ttrain-rmse:0.08843\tval-rmse:0.02165\n",
      "[18]\ttrain-rmse:0.08438\tval-rmse:0.02127\n",
      "[19]\ttrain-rmse:0.08049\tval-rmse:0.02082\n",
      "[20]\ttrain-rmse:0.07680\tval-rmse:0.02060\n",
      "[21]\ttrain-rmse:0.07325\tval-rmse:0.02030\n",
      "[22]\ttrain-rmse:0.06996\tval-rmse:0.02007\n",
      "[23]\ttrain-rmse:0.06684\tval-rmse:0.01997\n",
      "[24]\ttrain-rmse:0.06388\tval-rmse:0.01975\n",
      "[25]\ttrain-rmse:0.06107\tval-rmse:0.01934\n",
      "[26]\ttrain-rmse:0.05834\tval-rmse:0.01915\n",
      "[27]\ttrain-rmse:0.05579\tval-rmse:0.01905\n",
      "[28]\ttrain-rmse:0.05334\tval-rmse:0.01874\n",
      "[29]\ttrain-rmse:0.05101\tval-rmse:0.01861\n",
      "[30]\ttrain-rmse:0.04888\tval-rmse:0.01849\n",
      "[31]\ttrain-rmse:0.04680\tval-rmse:0.01836\n",
      "[32]\ttrain-rmse:0.04481\tval-rmse:0.01829\n",
      "[33]\ttrain-rmse:0.04294\tval-rmse:0.01821\n",
      "[34]\ttrain-rmse:0.04115\tval-rmse:0.01822\n",
      "[35]\ttrain-rmse:0.03951\tval-rmse:0.01821\n",
      "[36]\ttrain-rmse:0.03793\tval-rmse:0.01814\n",
      "[37]\ttrain-rmse:0.03644\tval-rmse:0.01819\n",
      "[38]\ttrain-rmse:0.03503\tval-rmse:0.01816\n",
      "[39]\ttrain-rmse:0.03369\tval-rmse:0.01810\n",
      "[40]\ttrain-rmse:0.03239\tval-rmse:0.01813\n",
      "[41]\ttrain-rmse:0.03118\tval-rmse:0.01812\n",
      "[42]\ttrain-rmse:0.03002\tval-rmse:0.01813\n",
      "[43]\ttrain-rmse:0.02895\tval-rmse:0.01811\n",
      "[44]\ttrain-rmse:0.02793\tval-rmse:0.01818\n",
      "[45]\ttrain-rmse:0.02697\tval-rmse:0.01826\n",
      "[46]\ttrain-rmse:0.02601\tval-rmse:0.01835\n",
      "[47]\ttrain-rmse:0.02513\tval-rmse:0.01838\n",
      "[48]\ttrain-rmse:0.02429\tval-rmse:0.01842\n",
      "[49]\ttrain-rmse:0.02352\tval-rmse:0.01839\n",
      "[50]\ttrain-rmse:0.02278\tval-rmse:0.01846\n",
      "[51]\ttrain-rmse:0.02208\tval-rmse:0.01842\n",
      "[52]\ttrain-rmse:0.02141\tval-rmse:0.01850\n",
      "[53]\ttrain-rmse:0.02083\tval-rmse:0.01849\n",
      "[54]\ttrain-rmse:0.02023\tval-rmse:0.01849\n",
      "[55]\ttrain-rmse:0.01966\tval-rmse:0.01849\n",
      "[56]\ttrain-rmse:0.01913\tval-rmse:0.01859\n",
      "[57]\ttrain-rmse:0.01862\tval-rmse:0.01861\n",
      "[58]\ttrain-rmse:0.01816\tval-rmse:0.01867\n",
      "[0]\ttrain-rmse:0.20076\tval-rmse:0.03624\n",
      "[1]\ttrain-rmse:0.19109\tval-rmse:0.03476\n",
      "[2]\ttrain-rmse:0.18192\tval-rmse:0.03355\n",
      "[3]\ttrain-rmse:0.17328\tval-rmse:0.03241\n",
      "[4]\ttrain-rmse:0.16511\tval-rmse:0.03138\n",
      "[5]\ttrain-rmse:0.15724\tval-rmse:0.03029\n",
      "[6]\ttrain-rmse:0.14988\tval-rmse:0.02939\n",
      "[7]\ttrain-rmse:0.14278\tval-rmse:0.02846\n",
      "[8]\ttrain-rmse:0.13605\tval-rmse:0.02767\n",
      "[9]\ttrain-rmse:0.12964\tval-rmse:0.02672\n",
      "[10]\ttrain-rmse:0.12351\tval-rmse:0.02618\n",
      "[11]\ttrain-rmse:0.11780\tval-rmse:0.02550\n",
      "[12]\ttrain-rmse:0.11225\tval-rmse:0.02480\n",
      "[13]\ttrain-rmse:0.10700\tval-rmse:0.02419\n",
      "[14]\ttrain-rmse:0.10204\tval-rmse:0.02374\n",
      "[15]\ttrain-rmse:0.09734\tval-rmse:0.02339\n",
      "[16]\ttrain-rmse:0.09282\tval-rmse:0.02295\n",
      "[17]\ttrain-rmse:0.08853\tval-rmse:0.02268\n",
      "[18]\ttrain-rmse:0.08449\tval-rmse:0.02228\n",
      "[19]\ttrain-rmse:0.08061\tval-rmse:0.02193\n",
      "[20]\ttrain-rmse:0.07693\tval-rmse:0.02175\n",
      "[21]\ttrain-rmse:0.07341\tval-rmse:0.02150\n",
      "[22]\ttrain-rmse:0.07012\tval-rmse:0.02119\n",
      "[23]\ttrain-rmse:0.06697\tval-rmse:0.02112\n",
      "[24]\ttrain-rmse:0.06402\tval-rmse:0.02095\n",
      "[25]\ttrain-rmse:0.06122\tval-rmse:0.02075\n",
      "[26]\ttrain-rmse:0.05851\tval-rmse:0.02070\n",
      "[27]\ttrain-rmse:0.05595\tval-rmse:0.02069\n",
      "[28]\ttrain-rmse:0.05353\tval-rmse:0.02053\n",
      "[29]\ttrain-rmse:0.05120\tval-rmse:0.02048\n",
      "[30]\ttrain-rmse:0.04905\tval-rmse:0.02039\n",
      "[31]\ttrain-rmse:0.04702\tval-rmse:0.02030\n",
      "[32]\ttrain-rmse:0.04504\tval-rmse:0.02022\n",
      "[33]\ttrain-rmse:0.04315\tval-rmse:0.02016\n",
      "[34]\ttrain-rmse:0.04137\tval-rmse:0.02018\n",
      "[35]\ttrain-rmse:0.03972\tval-rmse:0.02015\n",
      "[36]\ttrain-rmse:0.03814\tval-rmse:0.02010\n",
      "[37]\ttrain-rmse:0.03665\tval-rmse:0.02020\n",
      "[38]\ttrain-rmse:0.03523\tval-rmse:0.02014\n",
      "[39]\ttrain-rmse:0.03389\tval-rmse:0.02012\n",
      "[40]\ttrain-rmse:0.03260\tval-rmse:0.02017\n",
      "[41]\ttrain-rmse:0.03142\tval-rmse:0.02017\n",
      "[42]\ttrain-rmse:0.03029\tval-rmse:0.02015\n",
      "[43]\ttrain-rmse:0.02921\tval-rmse:0.02011\n",
      "[44]\ttrain-rmse:0.02819\tval-rmse:0.02018\n",
      "[45]\ttrain-rmse:0.02723\tval-rmse:0.02029\n",
      "[46]\ttrain-rmse:0.02630\tval-rmse:0.02047\n",
      "[47]\ttrain-rmse:0.02543\tval-rmse:0.02043\n",
      "[48]\ttrain-rmse:0.02461\tval-rmse:0.02049\n",
      "[49]\ttrain-rmse:0.02387\tval-rmse:0.02048\n",
      "[50]\ttrain-rmse:0.02311\tval-rmse:0.02049\n",
      "[51]\ttrain-rmse:0.02242\tval-rmse:0.02042\n",
      "[52]\ttrain-rmse:0.02178\tval-rmse:0.02052\n",
      "[53]\ttrain-rmse:0.02117\tval-rmse:0.02049\n",
      "[54]\ttrain-rmse:0.02058\tval-rmse:0.02054\n",
      "[55]\ttrain-rmse:0.02005\tval-rmse:0.02050\n",
      "[0]\ttrain-rmse:0.20067\tval-rmse:0.03634\n",
      "[1]\ttrain-rmse:0.19104\tval-rmse:0.03503\n",
      "[2]\ttrain-rmse:0.18189\tval-rmse:0.03382\n",
      "[3]\ttrain-rmse:0.17328\tval-rmse:0.03268\n",
      "[4]\ttrain-rmse:0.16512\tval-rmse:0.03162\n",
      "[5]\ttrain-rmse:0.15728\tval-rmse:0.03071\n",
      "[6]\ttrain-rmse:0.14992\tval-rmse:0.02986\n",
      "[7]\ttrain-rmse:0.14282\tval-rmse:0.02901\n",
      "[8]\ttrain-rmse:0.13608\tval-rmse:0.02832\n",
      "[9]\ttrain-rmse:0.12971\tval-rmse:0.02746\n",
      "[10]\ttrain-rmse:0.12361\tval-rmse:0.02698\n",
      "[11]\ttrain-rmse:0.11790\tval-rmse:0.02634\n",
      "[12]\ttrain-rmse:0.11244\tval-rmse:0.02570\n",
      "[13]\ttrain-rmse:0.10721\tval-rmse:0.02534\n",
      "[14]\ttrain-rmse:0.10229\tval-rmse:0.02483\n",
      "[15]\ttrain-rmse:0.09763\tval-rmse:0.02455\n",
      "[16]\ttrain-rmse:0.09316\tval-rmse:0.02425\n",
      "[17]\ttrain-rmse:0.08886\tval-rmse:0.02408\n",
      "[18]\ttrain-rmse:0.08481\tval-rmse:0.02381\n",
      "[19]\ttrain-rmse:0.08095\tval-rmse:0.02347\n",
      "[20]\ttrain-rmse:0.07728\tval-rmse:0.02325\n",
      "[21]\ttrain-rmse:0.07378\tval-rmse:0.02308\n",
      "[22]\ttrain-rmse:0.07052\tval-rmse:0.02285\n",
      "[23]\ttrain-rmse:0.06739\tval-rmse:0.02281\n",
      "[24]\ttrain-rmse:0.06441\tval-rmse:0.02275\n",
      "[25]\ttrain-rmse:0.06162\tval-rmse:0.02280\n",
      "[26]\ttrain-rmse:0.05892\tval-rmse:0.02267\n",
      "[27]\ttrain-rmse:0.05636\tval-rmse:0.02266\n",
      "[28]\ttrain-rmse:0.05397\tval-rmse:0.02254\n",
      "[29]\ttrain-rmse:0.05164\tval-rmse:0.02253\n",
      "[30]\ttrain-rmse:0.04949\tval-rmse:0.02238\n",
      "[31]\ttrain-rmse:0.04742\tval-rmse:0.02230\n",
      "[32]\ttrain-rmse:0.04546\tval-rmse:0.02225\n",
      "[33]\ttrain-rmse:0.04359\tval-rmse:0.02214\n",
      "[34]\ttrain-rmse:0.04184\tval-rmse:0.02216\n",
      "[35]\ttrain-rmse:0.04022\tval-rmse:0.02219\n",
      "[36]\ttrain-rmse:0.03864\tval-rmse:0.02219\n",
      "[37]\ttrain-rmse:0.03714\tval-rmse:0.02232\n",
      "[38]\ttrain-rmse:0.03572\tval-rmse:0.02232\n",
      "[39]\ttrain-rmse:0.03439\tval-rmse:0.02229\n",
      "[40]\ttrain-rmse:0.03311\tval-rmse:0.02234\n",
      "[41]\ttrain-rmse:0.03195\tval-rmse:0.02236\n",
      "[42]\ttrain-rmse:0.03084\tval-rmse:0.02231\n",
      "[43]\ttrain-rmse:0.02977\tval-rmse:0.02229\n",
      "[44]\ttrain-rmse:0.02875\tval-rmse:0.02232\n",
      "[45]\ttrain-rmse:0.02777\tval-rmse:0.02241\n",
      "[46]\ttrain-rmse:0.02682\tval-rmse:0.02262\n",
      "[47]\ttrain-rmse:0.02596\tval-rmse:0.02266\n",
      "[48]\ttrain-rmse:0.02514\tval-rmse:0.02273\n",
      "[49]\ttrain-rmse:0.02438\tval-rmse:0.02277\n",
      "[50]\ttrain-rmse:0.02363\tval-rmse:0.02280\n",
      "[51]\ttrain-rmse:0.02295\tval-rmse:0.02276\n",
      "[52]\ttrain-rmse:0.02233\tval-rmse:0.02282\n",
      "[0]\ttrain-rmse:0.20057\tval-rmse:0.03662\n",
      "[1]\ttrain-rmse:0.19101\tval-rmse:0.03547\n",
      "[2]\ttrain-rmse:0.18190\tval-rmse:0.03445\n",
      "[3]\ttrain-rmse:0.17330\tval-rmse:0.03345\n",
      "[4]\ttrain-rmse:0.16515\tval-rmse:0.03238\n",
      "[5]\ttrain-rmse:0.15733\tval-rmse:0.03166\n",
      "[6]\ttrain-rmse:0.15000\tval-rmse:0.03094\n",
      "[7]\ttrain-rmse:0.14296\tval-rmse:0.03002\n",
      "[8]\ttrain-rmse:0.13627\tval-rmse:0.02946\n",
      "[9]\ttrain-rmse:0.12989\tval-rmse:0.02874\n",
      "[10]\ttrain-rmse:0.12380\tval-rmse:0.02834\n",
      "[11]\ttrain-rmse:0.11804\tval-rmse:0.02782\n",
      "[12]\ttrain-rmse:0.11251\tval-rmse:0.02739\n",
      "[13]\ttrain-rmse:0.10730\tval-rmse:0.02713\n",
      "[14]\ttrain-rmse:0.10238\tval-rmse:0.02671\n",
      "[15]\ttrain-rmse:0.09771\tval-rmse:0.02639\n",
      "[16]\ttrain-rmse:0.09322\tval-rmse:0.02613\n",
      "[17]\ttrain-rmse:0.08895\tval-rmse:0.02583\n",
      "[18]\ttrain-rmse:0.08490\tval-rmse:0.02575\n",
      "[19]\ttrain-rmse:0.08104\tval-rmse:0.02561\n",
      "[20]\ttrain-rmse:0.07736\tval-rmse:0.02543\n",
      "[21]\ttrain-rmse:0.07388\tval-rmse:0.02527\n",
      "[22]\ttrain-rmse:0.07061\tval-rmse:0.02516\n",
      "[23]\ttrain-rmse:0.06747\tval-rmse:0.02522\n",
      "[24]\ttrain-rmse:0.06449\tval-rmse:0.02512\n",
      "[25]\ttrain-rmse:0.06172\tval-rmse:0.02505\n",
      "[26]\ttrain-rmse:0.05902\tval-rmse:0.02508\n",
      "[27]\ttrain-rmse:0.05648\tval-rmse:0.02511\n",
      "[28]\ttrain-rmse:0.05408\tval-rmse:0.02513\n",
      "[29]\ttrain-rmse:0.05177\tval-rmse:0.02520\n",
      "[30]\ttrain-rmse:0.04964\tval-rmse:0.02512\n",
      "[31]\ttrain-rmse:0.04757\tval-rmse:0.02506\n",
      "[32]\ttrain-rmse:0.04558\tval-rmse:0.02499\n",
      "[33]\ttrain-rmse:0.04374\tval-rmse:0.02491\n",
      "[34]\ttrain-rmse:0.04201\tval-rmse:0.02483\n",
      "[35]\ttrain-rmse:0.04040\tval-rmse:0.02491\n",
      "[36]\ttrain-rmse:0.03883\tval-rmse:0.02487\n",
      "[37]\ttrain-rmse:0.03735\tval-rmse:0.02501\n",
      "[38]\ttrain-rmse:0.03594\tval-rmse:0.02509\n",
      "[39]\ttrain-rmse:0.03461\tval-rmse:0.02505\n",
      "[40]\ttrain-rmse:0.03334\tval-rmse:0.02511\n",
      "[41]\ttrain-rmse:0.03213\tval-rmse:0.02517\n",
      "[42]\ttrain-rmse:0.03103\tval-rmse:0.02517\n",
      "[43]\ttrain-rmse:0.02996\tval-rmse:0.02511\n",
      "[44]\ttrain-rmse:0.02895\tval-rmse:0.02512\n",
      "[45]\ttrain-rmse:0.02799\tval-rmse:0.02524\n",
      "[46]\ttrain-rmse:0.02702\tval-rmse:0.02528\n",
      "[47]\ttrain-rmse:0.02618\tval-rmse:0.02534\n",
      "[48]\ttrain-rmse:0.02537\tval-rmse:0.02539\n",
      "[49]\ttrain-rmse:0.02464\tval-rmse:0.02548\n",
      "[50]\ttrain-rmse:0.02392\tval-rmse:0.02549\n",
      "[51]\ttrain-rmse:0.02326\tval-rmse:0.02543\n",
      "[52]\ttrain-rmse:0.02259\tval-rmse:0.02563\n",
      "[53]\ttrain-rmse:0.02199\tval-rmse:0.02575\n",
      "[54]\ttrain-rmse:0.02136\tval-rmse:0.02574\n",
      "[0]\ttrain-rmse:0.20042\tval-rmse:0.03662\n",
      "[1]\ttrain-rmse:0.19090\tval-rmse:0.03557\n",
      "[2]\ttrain-rmse:0.18181\tval-rmse:0.03474\n",
      "[3]\ttrain-rmse:0.17323\tval-rmse:0.03382\n",
      "[4]\ttrain-rmse:0.16510\tval-rmse:0.03277\n",
      "[5]\ttrain-rmse:0.15732\tval-rmse:0.03187\n",
      "[6]\ttrain-rmse:0.14997\tval-rmse:0.03123\n",
      "[7]\ttrain-rmse:0.14296\tval-rmse:0.03041\n",
      "[8]\ttrain-rmse:0.13629\tval-rmse:0.02971\n",
      "[9]\ttrain-rmse:0.12993\tval-rmse:0.02913\n",
      "[10]\ttrain-rmse:0.12388\tval-rmse:0.02867\n",
      "[11]\ttrain-rmse:0.11812\tval-rmse:0.02824\n",
      "[12]\ttrain-rmse:0.11264\tval-rmse:0.02771\n",
      "[13]\ttrain-rmse:0.10744\tval-rmse:0.02749\n",
      "[14]\ttrain-rmse:0.10250\tval-rmse:0.02718\n",
      "[15]\ttrain-rmse:0.09785\tval-rmse:0.02682\n",
      "[16]\ttrain-rmse:0.09339\tval-rmse:0.02670\n",
      "[17]\ttrain-rmse:0.08912\tval-rmse:0.02656\n",
      "[18]\ttrain-rmse:0.08509\tval-rmse:0.02662\n",
      "[19]\ttrain-rmse:0.08125\tval-rmse:0.02664\n",
      "[20]\ttrain-rmse:0.07757\tval-rmse:0.02664\n",
      "[21]\ttrain-rmse:0.07410\tval-rmse:0.02656\n",
      "[22]\ttrain-rmse:0.07087\tval-rmse:0.02652\n",
      "[23]\ttrain-rmse:0.06777\tval-rmse:0.02652\n",
      "[24]\ttrain-rmse:0.06485\tval-rmse:0.02645\n",
      "[25]\ttrain-rmse:0.06207\tval-rmse:0.02644\n",
      "[26]\ttrain-rmse:0.05938\tval-rmse:0.02646\n",
      "[27]\ttrain-rmse:0.05681\tval-rmse:0.02650\n",
      "[28]\ttrain-rmse:0.05441\tval-rmse:0.02650\n",
      "[29]\ttrain-rmse:0.05211\tval-rmse:0.02665\n",
      "[30]\ttrain-rmse:0.04997\tval-rmse:0.02655\n",
      "[31]\ttrain-rmse:0.04786\tval-rmse:0.02651\n",
      "[32]\ttrain-rmse:0.04588\tval-rmse:0.02644\n",
      "[33]\ttrain-rmse:0.04401\tval-rmse:0.02636\n",
      "[34]\ttrain-rmse:0.04229\tval-rmse:0.02629\n",
      "[35]\ttrain-rmse:0.04069\tval-rmse:0.02636\n",
      "[36]\ttrain-rmse:0.03910\tval-rmse:0.02637\n",
      "[37]\ttrain-rmse:0.03763\tval-rmse:0.02648\n",
      "[38]\ttrain-rmse:0.03618\tval-rmse:0.02652\n",
      "[39]\ttrain-rmse:0.03485\tval-rmse:0.02653\n",
      "[40]\ttrain-rmse:0.03358\tval-rmse:0.02655\n",
      "[41]\ttrain-rmse:0.03238\tval-rmse:0.02671\n",
      "[42]\ttrain-rmse:0.03125\tval-rmse:0.02685\n",
      "[43]\ttrain-rmse:0.03017\tval-rmse:0.02690\n",
      "[44]\ttrain-rmse:0.02914\tval-rmse:0.02694\n",
      "[45]\ttrain-rmse:0.02817\tval-rmse:0.02703\n",
      "[46]\ttrain-rmse:0.02724\tval-rmse:0.02709\n",
      "[47]\ttrain-rmse:0.02638\tval-rmse:0.02718\n",
      "[48]\ttrain-rmse:0.02561\tval-rmse:0.02727\n",
      "[49]\ttrain-rmse:0.02486\tval-rmse:0.02742\n",
      "[50]\ttrain-rmse:0.02411\tval-rmse:0.02739\n",
      "[51]\ttrain-rmse:0.02341\tval-rmse:0.02740\n",
      "[52]\ttrain-rmse:0.02275\tval-rmse:0.02737\n",
      "[53]\ttrain-rmse:0.02217\tval-rmse:0.02746\n",
      "[0]\ttrain-rmse:0.20031\tval-rmse:0.03679\n",
      "[1]\ttrain-rmse:0.19079\tval-rmse:0.03568\n",
      "[2]\ttrain-rmse:0.18174\tval-rmse:0.03485\n",
      "[3]\ttrain-rmse:0.17310\tval-rmse:0.03384\n",
      "[4]\ttrain-rmse:0.16505\tval-rmse:0.03300\n",
      "[5]\ttrain-rmse:0.15729\tval-rmse:0.03229\n",
      "[6]\ttrain-rmse:0.14997\tval-rmse:0.03171\n",
      "[7]\ttrain-rmse:0.14295\tval-rmse:0.03096\n",
      "[8]\ttrain-rmse:0.13630\tval-rmse:0.03058\n",
      "[9]\ttrain-rmse:0.12993\tval-rmse:0.03004\n",
      "[10]\ttrain-rmse:0.12385\tval-rmse:0.02976\n",
      "[11]\ttrain-rmse:0.11808\tval-rmse:0.02957\n",
      "[12]\ttrain-rmse:0.11261\tval-rmse:0.02912\n",
      "[13]\ttrain-rmse:0.10743\tval-rmse:0.02891\n",
      "[14]\ttrain-rmse:0.10253\tval-rmse:0.02861\n",
      "[15]\ttrain-rmse:0.09789\tval-rmse:0.02848\n",
      "[16]\ttrain-rmse:0.09345\tval-rmse:0.02841\n",
      "[17]\ttrain-rmse:0.08921\tval-rmse:0.02833\n",
      "[18]\ttrain-rmse:0.08518\tval-rmse:0.02841\n",
      "[19]\ttrain-rmse:0.08136\tval-rmse:0.02846\n",
      "[20]\ttrain-rmse:0.07767\tval-rmse:0.02847\n",
      "[21]\ttrain-rmse:0.07417\tval-rmse:0.02835\n",
      "[22]\ttrain-rmse:0.07095\tval-rmse:0.02850\n",
      "[23]\ttrain-rmse:0.06785\tval-rmse:0.02861\n",
      "[24]\ttrain-rmse:0.06494\tval-rmse:0.02852\n",
      "[25]\ttrain-rmse:0.06216\tval-rmse:0.02847\n",
      "[26]\ttrain-rmse:0.05949\tval-rmse:0.02840\n",
      "[27]\ttrain-rmse:0.05691\tval-rmse:0.02851\n",
      "[28]\ttrain-rmse:0.05450\tval-rmse:0.02840\n",
      "[29]\ttrain-rmse:0.05219\tval-rmse:0.02845\n",
      "[30]\ttrain-rmse:0.05004\tval-rmse:0.02839\n",
      "[31]\ttrain-rmse:0.04800\tval-rmse:0.02836\n",
      "[32]\ttrain-rmse:0.04602\tval-rmse:0.02837\n",
      "[33]\ttrain-rmse:0.04416\tval-rmse:0.02829\n",
      "[34]\ttrain-rmse:0.04244\tval-rmse:0.02817\n",
      "[35]\ttrain-rmse:0.04086\tval-rmse:0.02823\n",
      "[36]\ttrain-rmse:0.03928\tval-rmse:0.02825\n",
      "[37]\ttrain-rmse:0.03779\tval-rmse:0.02827\n",
      "[38]\ttrain-rmse:0.03637\tval-rmse:0.02828\n",
      "[39]\ttrain-rmse:0.03505\tval-rmse:0.02832\n",
      "[40]\ttrain-rmse:0.03379\tval-rmse:0.02841\n",
      "[41]\ttrain-rmse:0.03259\tval-rmse:0.02853\n",
      "[42]\ttrain-rmse:0.03146\tval-rmse:0.02873\n",
      "[43]\ttrain-rmse:0.03038\tval-rmse:0.02869\n",
      "[44]\ttrain-rmse:0.02939\tval-rmse:0.02875\n",
      "[45]\ttrain-rmse:0.02845\tval-rmse:0.02882\n",
      "[46]\ttrain-rmse:0.02751\tval-rmse:0.02895\n",
      "[47]\ttrain-rmse:0.02666\tval-rmse:0.02899\n",
      "[48]\ttrain-rmse:0.02585\tval-rmse:0.02910\n",
      "[49]\ttrain-rmse:0.02510\tval-rmse:0.02924\n",
      "[50]\ttrain-rmse:0.02436\tval-rmse:0.02925\n",
      "[51]\ttrain-rmse:0.02369\tval-rmse:0.02929\n",
      "[52]\ttrain-rmse:0.02304\tval-rmse:0.02944\n",
      "[53]\ttrain-rmse:0.02243\tval-rmse:0.02953\n",
      "[54]\ttrain-rmse:0.02181\tval-rmse:0.02962\n",
      "[0]\ttrain-rmse:0.20018\tval-rmse:0.03707\n",
      "[1]\ttrain-rmse:0.19070\tval-rmse:0.03623\n",
      "[2]\ttrain-rmse:0.18168\tval-rmse:0.03527\n",
      "[3]\ttrain-rmse:0.17305\tval-rmse:0.03422\n",
      "[4]\ttrain-rmse:0.16491\tval-rmse:0.03350\n",
      "[5]\ttrain-rmse:0.15718\tval-rmse:0.03288\n",
      "[6]\ttrain-rmse:0.14986\tval-rmse:0.03232\n",
      "[7]\ttrain-rmse:0.14285\tval-rmse:0.03172\n",
      "[8]\ttrain-rmse:0.13623\tval-rmse:0.03114\n",
      "[9]\ttrain-rmse:0.12989\tval-rmse:0.03074\n",
      "[10]\ttrain-rmse:0.12382\tval-rmse:0.03053\n",
      "[11]\ttrain-rmse:0.11804\tval-rmse:0.03034\n",
      "[12]\ttrain-rmse:0.11254\tval-rmse:0.02992\n",
      "[13]\ttrain-rmse:0.10738\tval-rmse:0.02974\n",
      "[14]\ttrain-rmse:0.10248\tval-rmse:0.02957\n",
      "[15]\ttrain-rmse:0.09782\tval-rmse:0.02954\n",
      "[16]\ttrain-rmse:0.09337\tval-rmse:0.02934\n",
      "[17]\ttrain-rmse:0.08918\tval-rmse:0.02906\n",
      "[18]\ttrain-rmse:0.08517\tval-rmse:0.02920\n",
      "[19]\ttrain-rmse:0.08138\tval-rmse:0.02926\n",
      "[20]\ttrain-rmse:0.07773\tval-rmse:0.02934\n",
      "[21]\ttrain-rmse:0.07425\tval-rmse:0.02926\n",
      "[22]\ttrain-rmse:0.07103\tval-rmse:0.02939\n",
      "[23]\ttrain-rmse:0.06793\tval-rmse:0.02941\n",
      "[24]\ttrain-rmse:0.06500\tval-rmse:0.02931\n",
      "[25]\ttrain-rmse:0.06221\tval-rmse:0.02930\n",
      "[26]\ttrain-rmse:0.05954\tval-rmse:0.02935\n",
      "[27]\ttrain-rmse:0.05704\tval-rmse:0.02944\n",
      "[28]\ttrain-rmse:0.05463\tval-rmse:0.02938\n",
      "[29]\ttrain-rmse:0.05237\tval-rmse:0.02932\n",
      "[30]\ttrain-rmse:0.05023\tval-rmse:0.02935\n",
      "[31]\ttrain-rmse:0.04819\tval-rmse:0.02944\n",
      "[32]\ttrain-rmse:0.04621\tval-rmse:0.02946\n",
      "[33]\ttrain-rmse:0.04434\tval-rmse:0.02960\n",
      "[34]\ttrain-rmse:0.04262\tval-rmse:0.02958\n",
      "[35]\ttrain-rmse:0.04105\tval-rmse:0.02965\n",
      "[36]\ttrain-rmse:0.03953\tval-rmse:0.02954\n"
     ]
    }
   ],
   "source": [
    "forecast_horizon = 14\n",
    "lookback = 90\n",
    "\n",
    "xgb_features_idx = [(raw_features + engineered_features).index(f) for f in engineered_features ]\n",
    "\n",
    "X_train_xgb = X_train[:,xgb_features_idx]\n",
    "X_val_xgb = X_val[:,xgb_features_idx]\n",
    "X_test_xgb = X_test[:,xgb_features_idx]\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"eta\": 0.05,\n",
    "    \"max_depth\": 6,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"min_child_weight\": 3,\n",
    "    \"gamma\": 0,\n",
    "    \"lambda\": 1,\n",
    "    \"alpha\": 0,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "xgb_models = []\n",
    "y_preds_xgb = []\n",
    "\n",
    "for step in range(forecast_horizon):\n",
    "    # target is shifted for each horizon step\n",
    "    y_train_step = y_train[lookback + step: len(y_train) - forecast_horizon + step + 1]\n",
    "    y_val_step   = y_val[lookback + step: len(y_val) - forecast_horizon + step + 1]\n",
    "    y_test_step  = y_test[lookback + step: len(y_test) - forecast_horizon + step + 1]\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train_xgb[lookback:len(y_train_step)+lookback], label=y_train_step)\n",
    "    dval   = xgb.DMatrix(X_val_xgb[lookback:len(y_val_step)+lookback], label=y_val_step)\n",
    "    dtest  = xgb.DMatrix(X_test_xgb[lookback:len(y_test_step)+lookback])\n",
    "\n",
    "    model_step = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=500,\n",
    "        evals=[(dtrain, \"train\"), (dval, \"val\")],\n",
    "        early_stopping_rounds=20,\n",
    "        verbose_eval=True\n",
    "    )\n",
    "    xgb_models.append(model_step)\n",
    "    y_preds_xgb.append(model_step.predict(dtest))\n",
    "\n",
    "y_pred_xgb = np.column_stack(y_preds_xgb)  # shape: (samples, horizon)\n",
    "y_pred_xgb_inv = scaler_y.inverse_transform(y_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cef7f7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create True Multi-Step Targets for Evaluation ---\n",
    "y_true_inv = scaler_y.inverse_transform(y_test[:len(y_pred_xgb_inv)])\n",
    "y_true_horizon = []\n",
    "for step in range(forecast_horizon):\n",
    "    y_true_horizon.append(y_true_inv[lookback + step : len(y_true_inv) - forecast_horizon + step + 1, 0])\n",
    "y_true_horizon = np.column_stack(y_true_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a03797bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step-wise Evaluation ===\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [26, 129]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m y_true_step = y_true_horizon[:, step]\n\u001b[32m      5\u001b[39m y_pred_step = y_pred_xgb_inv[:, step]\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m rmse = \u001b[43mroot_mean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m#rmse = np.sqrt(root_mean_squared_error(y_true_step, y_pred_step))\u001b[39;00m\n\u001b[32m     10\u001b[39m mae = mean_absolute_error(y_true_step, y_pred_step)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\test2\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\test2\\venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:665\u001b[39m, in \u001b[36mroot_mean_squared_error\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Root mean squared error regression loss.\u001b[39;00m\n\u001b[32m    616\u001b[39m \n\u001b[32m    617\u001b[39m \u001b[33;03mRead more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    659\u001b[39m \u001b[33;03m0.822...\u001b[39;00m\n\u001b[32m    660\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    662\u001b[39m xp, _ = get_namespace(y_true, y_pred, sample_weight, multioutput)\n\u001b[32m    664\u001b[39m output_errors = xp.sqrt(\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m     \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mraw_values\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    667\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    668\u001b[39m )\n\u001b[32m    670\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(multioutput, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    671\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m multioutput == \u001b[33m\"\u001b[39m\u001b[33mraw_values\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\test2\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:191\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m global_skip_validation = get_config()[\u001b[33m\"\u001b[39m\u001b[33mskip_parameter_validation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m func_sig = signature(func)\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\test2\\venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:580\u001b[39m, in \u001b[36mmean_squared_error\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput)\u001b[39m\n\u001b[32m    530\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[32m    531\u001b[39m \n\u001b[32m    532\u001b[39m \u001b[33;03mRead more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    576\u001b[39m \u001b[33;03m0.825...\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    578\u001b[39m xp, _ = get_namespace(y_true, y_pred, sample_weight, multioutput)\n\u001b[32m    579\u001b[39m _, y_true, y_pred, sample_weight, multioutput = (\n\u001b[32m--> \u001b[39m\u001b[32m580\u001b[39m     \u001b[43m_check_reg_targets_with_floating_dtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m )\n\u001b[32m    584\u001b[39m output_errors = _average((y_true - y_pred) ** \u001b[32m2\u001b[39m, axis=\u001b[32m0\u001b[39m, weights=sample_weight)\n\u001b[32m    586\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(multioutput, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\test2\\venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:209\u001b[39m, in \u001b[36m_check_reg_targets_with_floating_dtype\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Ensures y_true, y_pred, and sample_weight correspond to same regression task.\u001b[39;00m\n\u001b[32m    161\u001b[39m \n\u001b[32m    162\u001b[39m \u001b[33;03mExtends `_check_reg_targets` by automatically selecting a suitable floating-point\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    205\u001b[39m \u001b[33;03m    correct keyword.\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    207\u001b[39m dtype_name = _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp=xp)\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m y_type, y_true, y_pred, sample_weight, multioutput = \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y_type, y_true, y_pred, sample_weight, multioutput\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\test2\\venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:114\u001b[39m, in \u001b[36m_check_reg_targets\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput, dtype, xp)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check that y_true, y_pred and sample_weight belong to the same regression task.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03mTo reduce redundancy when calling `_find_matching_floating_dtype`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    110\u001b[39m \u001b[33;03m    correct keyword.\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    112\u001b[39m xp, _ = get_namespace(y_true, y_pred, multioutput, xp=xp)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m y_true = check_array(y_true, ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=dtype)\n\u001b[32m    116\u001b[39m y_pred = check_array(y_pred, ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\test2\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:473\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    471\u001b[39m lengths = [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lengths)) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    474\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    475\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    476\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [26, 129]"
     ]
    }
   ],
   "source": [
    "# --- Evaluate Model Performance ---\n",
    "print(\"=== Step-wise Evaluation ===\")\n",
    "for step in range(forecast_horizon):\n",
    "    y_true_step = y_true_horizon[:, step]\n",
    "    y_pred_step = y_pred_xgb_inv[:, step]\n",
    "\n",
    "    rmse = root_mean_squared_error(y_true_step, y_pred_step)\n",
    "\n",
    "    #rmse = np.sqrt(root_mean_squared_error(y_true_step, y_pred_step))\n",
    "    mae = mean_absolute_error(y_true_step, y_pred_step)\n",
    "    r2 = r2_score(y_true_step, y_pred_step)\n",
    "\n",
    "    print(f\"Step {step+1:2d}: RMSE={rmse:.4f}, MAE={mae:.4f}, RÂ²={r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25666db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "y_true_inv = scaler_y.inverse_transform(y_test[:len(y_pred_xgb_inv)])\n",
    "y_true_flat = y_true_inv.flatten()\n",
    "y_pred_flat = y_pred_xgb_inv.flatten()\n",
    "\n",
    "rmse = np.sqrt(root_mean_squared_error(y_true_inv, y_pred_xgb_inv))\n",
    "mae = mean_absolute_error(y_true_inv, y_pred_xgb_inv)\n",
    "r2 = r2_score(y_true_inv, y_pred_xgb_inv)\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}, MAE: {mae:.4f}, RÂ²: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac54af1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edae9b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(y_true_inv, label=\"Actual\")\n",
    "plt.plot(y_pred_xgb_inv, label=\"XGBoost Prediction\")\n",
    "plt.legend()\n",
    "plt.title(\"XGBoost Forecast vs Actual\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
